{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/nmquy/KEPCO/kepco/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 05:45:07.229463: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-12 05:45:07.251842: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-12 05:45:07.258495: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-12 05:45:07.274722: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-12 05:45:09.970765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/nmquy/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.586 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir=\"/workspace/nmquy/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Danh s√°ch nh√¢n vi√™n nh√† h√†ng</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h1> Danh s√°ch nh√¢n vi√™n nh√† h√†ng </h1>\n",
      "    <table border=\"1\">\n",
      "        <tr>\n",
      "            <th>T√™n</th>\n",
      "            <th>Email</th>\n",
      "        </tr>\n",
      "        {% for employee in data %}\n",
      "        <tr>\n",
      "            <td>{{ employee['name'] }}</td>\n",
      "            <td>{{ employee['email'] }}</td>\n",
      "        </tr>\n",
      "        {% endfor %}\n",
      "    </table>\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "### L·ªùi gi·∫£i:\n",
      "- ƒê·∫ßu ti√™n, ta c·∫ßn chuy·ªÉn dictionary t·ª´ JSON sang b·∫£ng HTML c√≥ header v√† title c·ªôt.\n",
      "- Sau ƒë√≥, ta s·∫Ω s·ª≠ d·ª•ng m√£ HTML ƒë·ªÉ t·∫°o ra m·ªôt b·∫£ng v·ªõi c√°c c·ªôt t√™n v√† email.\n",
      "- T·∫•t c·∫£ c√°c nh√¢n vi√™n s·∫Ω ƒë∆∞·ª£c li·ªát k√™ d∆∞·ªõi d·∫°ng c√°c h√†ng trong b·∫£ng.\n",
      "- ·ªû m·ªói h√†ng, s·∫Ω c√≥ hai √¥, m·ªôt √¥ cho t√™n v√† m·ªôt √¥ cho email c·ªßa nh√¢n vi√™n.\n",
      "\n",
      "### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Chuy·ªÉn dictionary python sau t·ª´ JSON sang b·∫£ng HTML c√≥ header v√† title c·ªôt. Ch·ªâ s·ª≠ d·ª•ng m√£ HTML:\"\n",
    "input_data = '''\n",
    "{'resturant employees': [{'name': 'Shyam', 'email': 'shyamjaiswal@gmail.com'}, {'name': 'Bob', 'email': 'bob32@gmail.com'}, {'name': 'Jai', 'email': 'jai87@gmail.com'}]}\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot rewrite question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a rewritten version of the question that's concise but provides enough context for those without knowledge of the previous conversation:\n",
      "\n",
      "\"What NLP tasks can the BERT model perform well, given its training on a large corpus of internet text?\"\n",
      "\n",
      "This rewritten question maintains the essence of the original question while providing a brief background on the BERT model, ensuring that anyone reading it can understand the context and requirements of the task.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"You are able to reason from previous conversation and the recent question, to come up with a rewrite of the question which is concise but with enough information that people without knowledge of previous conversation can understand the question:\"\n",
    "input_data = '''\n",
    "## Previous conversation\n",
    "user: What is BERT?\n",
    "assistant: BERT stands for \"Bidirectional Encoder Representations from Transformers.\" It is a natural language processing (NLP) model developed by Google. \n",
    "user: What data was used for its training?\n",
    "assistant: The BERT (Bidirectional Encoder Representations from Transformers) model was trained on a large corpus of publicly available text from the internet. It was trained on a combination of books, articles, websites, and other sources to learn the language patterns and relationships between words.\n",
    "## Question\n",
    "user: What NLP tasks can it perform well?\n",
    "## Rewritten question\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B∆∞·ªõc 1 - L·∫•y m·ªôt s·ªë n∆∞·ªõc s√¥i.\n",
      "B∆∞·ªõc 2 - L·∫•y m·ªôt chi·∫øc c·ªëc v√† ƒë·∫∑t m·ªôt t√∫i tr√† v√†o ƒë√≥.\n",
      "B∆∞·ªõc 3 - ƒê·ªï n∆∞·ªõc n√≥ng l√™n t√∫i tr√†.\n",
      "B∆∞·ªõc 4 - ƒê·ªÉ tr√† ng√¢m trong v√†i ph√∫t.\n",
      "B∆∞·ªõc 5 - L·∫•y t√∫i tr√† ra.\n",
      "B∆∞·ªõc 6 - Th√™m m·ªôt √≠t ƒë∆∞·ªùng ho·∫∑c s·ªØa n·∫øu th√≠ch.  B∆∞·ªõc 7 - T√°ch tr√† ƒë√£ s·∫µn s√†ng ƒë·ªÉ th∆∞·ªüng th·ª©c.  (C√≥ th·ªÉ th√™m b∆∞·ªõc n√†y n·∫øu b·∫°n mu·ªën c√≥ m·ªôt t√°ch tr√† ho√†n h·∫£o) - Ki·ªÉm tra l·∫°i xem c√≥ c·∫ßn ƒëi·ªÅu ch·ªânh h∆∞∆°ng v·ªã hay kh√¥ng.  (C√≥ th·ªÉ th√™m b∆∞·ªõc n√†y n·∫øu b·∫°n mu·ªën c√≥ m·ªôt t√°ch tr√† ho√†n h·∫£o) - L·∫•y m·ªôt chi·∫øc ch√©n ho·∫∑c t√°ch tr√† v√† th∆∞·ªüng th·ª©c.   (C√≥ th·ªÉ th√™m b∆∞·ªõc n√†y n·∫øu b·∫°n mu·ªën c√≥ m·ªôt t√°ch tr√† ho√†n h·∫£o) - Th∆∞·ªüng th·ª©c t√°ch tr√† c·ªßa b·∫°n v√† t·∫≠n h∆∞·ªüng h∆∞∆°ng v·ªã.   (C√≥ th·ªÉ th√™m b∆∞·ªõc n√†y n·∫øu b·∫°n mu·ªën c√≥ m·ªôt t√°ch tr√† ho√†n h·∫£o) - N·∫øu b·∫°n mu·ªën c√≥ m·ªôt t√°ch tr√† ƒë·∫∑c bi·ªát, b·∫°n c√≥ th·ªÉ th√™m m·ªôt s·ªë nguy√™n li·ªáu kh√°c nh∆∞ hoa qu·∫£, kem, ho·∫∑c c√°c lo·∫°i gia v·ªã kh√°c.   (C√≥ th·ªÉ th√™m b∆∞·ªõc n√†y n·∫øu b·∫°n mu·ªën c√≥ m·ªôt t√°ch tr√† ho√†n h·∫£o) - Th\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p vƒÉn b·∫£n. N·∫øu n√≥ ch·ª©a m·ªôt chu·ªói c√°c h∆∞·ªõng d·∫´n, vi·∫øt l·∫°i c√°c h∆∞·ªõng d·∫´n ƒë√≥ theo ƒë·ªãnh d·∫°ng sau:\n",
    "B∆∞·ªõc 1 - ...\n",
    "B∆∞·ªõc 2 -‚Ä¶\n",
    "‚Ä¶\n",
    "B∆∞·ªõc N -‚Ä¶\n",
    "\n",
    "N·∫øu vƒÉn b·∫£n kh√¥ng ch·ª©a m·ªôt chu·ªói h∆∞·ªõng d·∫´n, sau ƒë√≥ ch·ªâ c·∫ßn vi·∫øt \"No steps provided.\\\"\"\"\"\n",
    "input_data = '''\n",
    "L√†m m·ªôt t√°ch tr√† th·∫≠t d·ªÖ d√†ng! ƒê·∫ßu ti√™n, b·∫°n c·∫ßn l·∫•y m·ªôt s·ªë  n∆∞·ªõc s√¥i. Trong khi ƒëi·ªÅu ƒë√≥ ƒëang x·∫£y ra,  \n",
    "l·∫•y m·ªôt chi·∫øc c·ªëc v√† ƒë·∫∑t m·ªôt t√∫i tr√† v√†o ƒë√≥. M·ªôt khi n∆∞·ªõc ƒë√£ ƒë·ªß n√≥ng, ch·ªâ c·∫ßn ƒë·ªï n√≥ l√™n t√∫i tr√†.  \n",
    "H√£y ƒë·ªÉ n√≥ ng·ªìi m·ªôt ch√∫t ƒë·ªÉ tr√† c√≥ th·ªÉ ng√¢m. Sau m·ªôt v√†i ph√∫t, l·∫•y t√∫i tr√† ra. N·∫øu b·∫°n  \n",
    "nh∆∞, b·∫°n c√≥ th·ªÉ th√™m m·ªôt √≠t ƒë∆∞·ªùng ho·∫∑c s·ªØa cho v·ª´a ƒÉn. V√† th·∫ø l√† xong! B·∫°n ƒë√£ c√≥ cho m√¨nh m·ªôt m√≥n ngon t√°ch tr√† ƒë·ªÉ th∆∞·ªüng th·ª©c.\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow many instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \n",
      "V·ª• ch√°y x·∫£y ra t·∫°i nh√† kho ch·ª©a ƒë·ªì g·ªó ·ªü x√£ C∆∞∆°ng S∆°n, huy·ªán L·ª•c Nam, t·ªânh B·∫Øc Giang. Ng·ªçn l·ª≠a b√πng l√™n v√† lan nhanh, khi·∫øn h·∫ßu h·∫øt t√†i s·∫£n b√™n trong b·ªã thi√™u r·ª•i. 40 chi·∫øn sƒ© ƒë√£ d·∫≠p t·∫Øt ng·ªçn l·ª≠a sau kho·∫£ng 40 ph√∫t, nh∆∞ng kh√¥ng g√¢y thi·ªát h·∫°i v·ªÅ ng∆∞·ªùi.\n",
      "\n",
      "2. \n",
      "A fire broke out at a wood storage warehouse in C∆∞∆°ng S∆°n, L·ª•c Nam, B·∫Øc Giang province. The flames spread quickly, destroying most of the property inside. 40 firefighters extinguished the blaze after about 40 minutes, but no one was injured.\n",
      "\n",
      "3. \n",
      "```json\n",
      "{\n",
      "  \"Vietnamese_summary\": \"V·ª• ch√°y x·∫£y ra t·∫°i nh√† kho ch·ª©a ƒë·ªì g·ªó ·ªü x√£ C∆∞∆°ng S∆°n, huy·ªán L·ª•c Nam, t·ªânh B·∫Øc Giang. Ng·ªçn l·ª≠a b√πng l√™n v√† lan nhanh, khi·∫øn h·∫ßu h·∫øt t√†i s·∫£n b√™n trong b·ªã thi√™u r·ª•i. 40 chi·∫øn sƒ© ƒë√£ d·∫≠p t·∫Øt ng·ªçn l·ª≠a sau kho·∫£ng 40 ph√∫t, nh∆∞ng kh√¥ng g√¢y thi·ªát h·∫°i v·ªÅ ng∆∞·ªùi.\",\n",
      "  \"English_summary\": \"A fire broke out at a wood storage warehouse in C∆∞∆°ng S∆°n, L·ª•c Nam, B·∫Øc Giang province. The flames spread quickly, destroying most of the property inside. 40 firefighters extinguished the blaze after about 40 minutes, but no one was injured.\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Tr√™n ƒë√¢y l√† c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n. H√£y li√™n h·ªá v·ªõi ch√∫ng t√¥i ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n v√† h·ªó tr·ª£ th√™m. C·∫£m ∆°n!. (H√¨nh ·∫£nh minh h·ªça:  https://danviet.vn ).................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Th·ª±c hi·ªán c√°c h√†nh ƒë·ªông sau: \n",
    "1 - T√≥m t·∫Øt ƒëo·∫°n vƒÉn b·∫£n sau.\n",
    "2 - D·ªãch t√≥m t·∫Øt sang ti·∫øng Anh.\n",
    "3 - Xu·∫•t ra m·ªôt ƒë·ªëi t∆∞·ª£ng json ch·ª©a c√°c kh√≥a sau: Vietnamese_summary, English_summary.\n",
    "T√°ch c√°c c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n b·∫±ng d·∫•u ng·∫Øt d√≤ng.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Kho·∫£ng 15h, nh√† kho ch·ª©a ƒë·ªì g·ªó C√¥ng ty TNHH V∆∞·ª£ng H√† ·ªü x√£ C∆∞∆°ng S∆°n, huy·ªán L·ª•c Nam b·ªëc ch√°y. M·ªôt ng∆∞·ªùi ƒë√†n √¥ng t·ª´ b√™n trong ch·∫°y ra ngo√†i h√¥ ho√°n nh·ªù h·ªó tr·ª£.\n",
    "\n",
    "Tuy nhi√™n do v·∫≠t li·ªáu d·ªÖ ch√°y n√™n ch·ªâ trong √≠t ph√∫t, ng·ªçn l·ª≠a ƒë√£ b√πng l√™n b·∫•t ch·∫•p n·ªó l·ª±c ch·ªØa ch√°y c·ªßa ng∆∞·ªùi d√¢n. Kho·∫£ng 15 ph√∫t sau, ng·ªçn l·ª≠a ƒë√£ lan ra to√†n b·ªô nh√† x∆∞·ªüng, c·ªôt c√≥i nghi ng√∫t b·ªëc cao h√†ng ch·ª•c m√©t.\n",
    "Ph√≤ng C·∫£nh s√°t ph√≤ng ch√°y, ch·ªØa ch√°y v√† c·ª©u n·∫°n c·ª©u h·ªô, C√¥ng an huy·ªán L·ª•c Nam ƒë√£ huy ƒë·ªông 40 chi·∫øn sƒ© c√πng ba xe ch·ªØa ch√°y ƒë·∫øn hi·ªán tr∆∞·ªùng. Sau kho·∫£ng 40 ph√∫t, ng·ªçn l·ª≠a ƒë∆∞·ª£c d·∫≠p t·∫Øt nh∆∞ng h·∫ßu h·∫øt t√†i s·∫£n b√™n trong b·ªã thi√™u r·ª•i.\n",
    "\n",
    "√îng Nguy·ªÖn M·∫°nh H∆∞ng, Ch·ªß t·ªãch UBND x√£ C∆∞∆°ng S∆°n, cho bi·∫øt v·ª• ch√°y kh√¥ng g√¢y thi·ªát h·∫°i v·ªÅ ng∆∞·ªùi. Tuy nhi√™n, x∆∞·ªüng g·ªó r·ªông kho·∫£ng 400 m2 n·∫±m s√°t ƒë∆∞·ªùng t·ªânh 293 ƒë√£ b·ªã thi√™u r·ª•i. To√†n b·ªô ph·∫ßn m√°i t√¥n v√† c·∫•u ki·ªán b√™n trong s·∫≠p ƒë·ªï, t∆∞·ªùng t√¥n b·ªã u·ªën cong, bi·∫øn d·∫°ng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gi·∫£i c·ªßa h·ªçc sinh ƒë√∫ng, v√¨ n√≥ ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n c√°c th√¥ng tin v√† y√™u c·∫ßu ƒë∆∞·ª£c cung c·∫•p. H·ªçc sinh ƒë√£ x√°c ƒë·ªãnh r√µ r√†ng c√°c chi ph√≠ li√™n quan ƒë·∫øn vi·ªác x√¢y d·ª±ng v√† b·∫£o tr√¨ t√≤a nh√† vƒÉn ph√≤ng, bao g·ªìm gi√° ƒë·∫•t, gi√° v·∫≠t li·ªáu x√¢y d·ª±ng v√† chi ph√≠ b·∫£o tr√¨. K·∫øt qu·∫£ cu·ªëi c√πng l√† m·ªôt h√†m c·ªßa di·ªán t√≠ch t√≤a nh√†, th·ªÉ hi·ªán t·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông nh∆∞ m·ªôt h√†m c·ªßa gi√° tr·ªã y. \n",
      "Gi·∫£i c·ªßa h·ªçc sinh ƒë√£ th·ªÉ hi·ªán s·ª± hi·ªÉu bi·∫øt v√† √°p d·ª•ng c√°c nguy√™n t·∫Øc kinh t·∫ø ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ th·ª±c t·∫ø, v√† do ƒë√≥, c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† m·ªôt c√°ch gi·∫£i quy·∫øt h·ª£p l·ªá v√† ch√≠nh x√°c.  Tuy nhi√™n, c·∫ßn l∆∞u √Ω r·∫±ng ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c, h·ªçc sinh n√™n s·ª≠ d·ª•ng ƒë∆°n v·ªã ti·ªÅn t·ªá ph√π h·ª£p (v√≠ d·ª•: USD) ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n v·ªÅ gi√° tr·ªã.  Ngo√†i ra, h·ªçc sinh c√≥ th·ªÉ mu·ªën xem x√©t c√°c chi ph√≠ kh√°c c√≥ th·ªÉ ph√°t sinh trong qu√° tr√¨nh x√¢y d·ª±ng v√† b·∫£o tr√¨ t√≤a nh√†, ch·∫≥ng h·∫°n nh∆∞ chi ph√≠ nh√¢n c√¥ng, chi ph√≠ thi·∫øt b·ªã ho·∫∑c chi ph√≠ qu·∫£n l√Ω kh√°c.  V·ªõi c√°c ƒëi·ªÅu ch·ªânh n√†y, gi·∫£i c·ªßa h·ªçc sinh s·∫Ω tr·ªü n√™n ch√≠nh x√°c v√† to√†n di·ªán h∆°n.  Tuy nhi√™n, theo y√™u c·∫ßu ban ƒë·∫ßu, gi·∫£i c·ªßa h·ªçc sinh ƒë√£ ƒë√°p ·ª©ng ƒë∆∞·ª£c y√™u c·∫ßu ch√≠nh x√°c v·ªÅ t√≠nh to√°n chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông c·ªßa t√≤a nh√† vƒÉn ph√≤ng.  V√¨ v·∫≠y, gi·∫£i c·ªßa h·ªçc sinh c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† ƒë√∫ng.  C√≥ th·ªÉ c√≥ m·ªôt s·ªë c√°ch gi·∫£i th√≠ch kh√°c, nh∆∞ng nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p, gi·∫£i c·ªßa h·ªçc sinh ƒë√£ ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.  V√¨ v·∫≠y, k·∫øt lu·∫≠n l√† gi·∫£i c·ªßa h·ªçc sinh l√† ƒë√∫ng.  C√≥ th·ªÉ c√≥ m·ªôt s·ªë c√°ch gi·∫£i th√≠ch kh√°c, nh∆∞ng nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p, gi·∫£i c·ªßa h·ªçc sinh ƒë√£ ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.  V√¨ v·∫≠y, k·∫øt lu·∫≠n l√† gi·∫£i c·ªßa h·ªçc sinh l√† ƒë√∫ng.  C√≥ th·ªÉ c√≥ m·ªôt s·ªë c√°ch gi·∫£i th√≠ch kh√°c, nh∆∞ng nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p, gi·∫£i c·ªßa h·ªçc sinh ƒë√£ ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.  V√¨ v·∫≠y, k·∫øt lu·∫≠n l√† gi·∫£i c·ªßa h·ªçc sinh l√† ƒë√∫ng.  C√≥ th·ªÉ c√≥ m·ªôt s·ªë c√°ch gi·∫£i th√≠ch kh√°c, nh∆∞ng nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p, gi·∫£i c·ªßa h·ªçc sinh ƒë√£ ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.  V√¨ v·∫≠y, k·∫øt lu·∫≠n l√† gi·∫£i c·ªßa h·ªçc sinh l√† ƒë√∫ng.  C√≥ th·ªÉ c√≥ m·ªôt s·ªë c√°ch gi·∫£i th√≠ch kh√°c, nh∆∞ng nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p,\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"X√°c ƒë·ªãnh xem c√°ch gi·∫£i quy·∫øt c·ªßa h·ªçc sinh c√≥ ƒë√∫ng hay kh√¥ng.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "C√¢u h·ªèi: T√¥i ƒëang l√™n k·∫ø ho·∫°ch x√¢y d·ª±ng m·ªôt t√≤a nh√† vƒÉn ph√≤ng v√† c·∫ßn t√≠nh to√°n chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông.\n",
    "\n",
    "Gi√° ƒë·∫•t l√† 200 USD/m2.\n",
    "T√¥i c√≥ th·ªÉ mua v·∫≠t li·ªáu x√¢y d·ª±ng v·ªõi gi√° 300 USD/m2.\n",
    "Chi ph√≠ b·∫£o tr√¨ h·∫±ng nƒÉm l√† 50 ngh√¨n USD cho to√†n b·ªô t√≤a nh√† v√† th√™m 15 USD/m2. T·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông l√† bao nhi√™u, nh∆∞ m·ªôt h√†m c·ªßa di·ªán t√≠ch t√≤a nh√† t√≠nh b·∫±ng m√©t vu√¥ng.\n",
    "Gi·∫£i ph√°p c·ªßa h·ªçc sinh: G·ªçi y l√† di·ªán t√≠ch c·ªßa t√≤a nh√† t√≠nh b·∫±ng m√©t vu√¥ng.\n",
    "\n",
    "Chi ph√≠:\n",
    "Gi√° ƒë·∫•t: 200y USD\n",
    "Gi√° v·∫≠t li·ªáu x√¢y d·ª±ng: 300y USD\n",
    "Chi ph√≠ b·∫£o tr√¨: 50.000+15y USD\n",
    "T·ªïng chi ph√≠: \n",
    "200y+300y+50.000+15y=515y+50.000 (USD)\n",
    "T·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n s·∫Ω l√† : 515y+50.000 USD, v·ªõi y l√† di·ªán t√≠ch c·ªßa t√≤a nh√†.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ƒêi·ªán tho·∫°i th√¥ng minh nh·∫≠n ƒë∆∞·ª£c nhi·ªÅu ƒë√°nh gi√° t√≠ch c·ª±c v·ªõi hi·ªáu nƒÉng m·∫°nh m·∫Ω, th·ªùi l∆∞·ª£ng pin ·∫•n t∆∞·ª£ng v√† t√≠nh nƒÉng b·∫£o m·∫≠t v√¢n tay ti·ªán l·ª£i.\"\n",
      "\n",
      "\n",
      "### Note:\n",
      "H√£y ƒë·∫£m b·∫£o r·∫±ng t√≥m t·∫Øt c·ªßa b·∫°n kh√¥ng qu√° 30 t·ª´. H√£y ch·ªçn nh·ªØng ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu ch√≠nh c·ªßa s·∫£n ph·∫©m ƒë·ªÉ th·ªÉ hi·ªán trong t√≥m t·∫Øt. H√£y s·ª≠ d·ª•ng ng√¥n ng·ªØ ƒë∆°n gi·∫£n v√† d·ªÖ hi·ªÉu ƒë·ªÉ t√≥m t·∫Øt ƒë∆∞·ª£c ng·∫Øn g·ªçn v√† d·ªÖ hi·ªÉu. H√£y ch·ªçn nh·ªØng t·ª´ kh√≥a ch√≠nh ƒë·ªÉ th·ªÉ hi·ªán trong t√≥m t·∫Øt, v√≠ d·ª• nh∆∞ \"hi·ªáu nƒÉng m·∫°nh m·∫Ω\", \"th·ªùi l∆∞·ª£ng pin ·∫•n t∆∞·ª£ng\",\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Nhi·ªám v·ª• c·ªßa b·∫°n l√† t·∫°o m·ªôt b·∫£n t√≥m t·∫Øt ng·∫Øn v·ªÅ ƒë√°nh gi√° s·∫£n ph·∫©m t·ª´ m·ªôt trang web th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. \n",
    "T√≥m t·∫Øt ƒë√°nh gi√° d∆∞·ªõi ƒë√¢y t·ªëi ƒëa 30 t·ª´.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Chi·∫øc ƒëi·ªán tho·∫°i th√¥ng minh nh·∫≠n ƒë∆∞·ª£c nhi·ªÅu ƒë√°nh gi√° t√≠ch c·ª±c t·ª´ kh√°ch h√†ng tr√™n trang web th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. H·ªç khen ng·ª£i hi·ªáu nƒÉng m·∫°nh m·∫Ω, t·ªëc ƒë·ªô x·ª≠ l√Ω nhanh v√† m∆∞·ª£t m√†, c√πng v·ªõi th·ªùi l∆∞·ª£ng pin ·∫•n t∆∞·ª£ng, c√≥ th·ªÉ s·ª≠ d·ª•ng c·∫£ ng√†y m√† kh√¥ng c·∫ßn s·∫°c. M√†n h√¨nh s·∫Øc n√©t v√† t√≠nh nƒÉng b·∫£o m·∫≠t v√¢n tay ti·ªán l·ª£i c≈©ng l√† nh·ªØng ƒëi·ªÉm c·ªông. Tuy nhi√™n, m·ªôt s·ªë kh√°ch h√†ng cho r·∫±ng camera ch∆∞a th·ª±c s·ª± t·ªët trong ƒëi·ªÅu ki·ªán thi·∫øu s√°ng, thi·∫øt k·∫ø c·ªßa ƒëi·ªán tho·∫°i h∆°i c·ªìng k·ªÅnh, v√† loa ngo√†i kh√¥ng ƒë·ªß l·ªõn. V·ªõi m·ª©c gi√° h·ª£p l√Ω, s·∫£n ph·∫©m n√†y v·∫´n ƒë∆∞·ª£c ƒë√°nh gi√° cao v·ªÅ ch·∫•t l∆∞·ª£ng t·ªïng th·ªÉ.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C·∫£m nh·∫≠n c·ªßa b√†i ƒë√°nh gi√° s·∫£n ph·∫©m sau ƒë√¢y l√† t√≠ch c·ª±c.\n",
      "\n",
      "\n",
      "### L·ªùi gi·∫£i th√≠ch:\n",
      "\n",
      "B√†i ƒë√°nh gi√° cho bi·∫øt ƒëi·ªán tho·∫°i \"ch·∫°y m∆∞·ª£t m√† v√† nhanh ch√≥ng khi s·ª≠ d·ª•ng c√°c ·ª©ng d·ª•ng n·∫∑ng\". ƒêi·ªÅu n√†y cho th·∫•y hi·ªáu nƒÉng c·ªßa s·∫£n ph·∫©m t·ªët v√† ƒë√°p ·ª©ng ƒë∆∞·ª£c nhu c·∫ßu c·ªßa ng∆∞·ªùi d√πng. V√¨ v·∫≠y, c·∫£m nh·∫≠n c·ªßa b√†i ƒë√°nh gi√° l√† t√≠ch c·ª±c.  [Gi·∫£i th√≠ch](https://khoahoc.tv/tap-hieu-nang-tuyet-vot-dien-thoai-chay-mut-ma-va-nhanh-nhon-khi-su-dung-cac-\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"C·∫£m nh·∫≠n c·ªßa b√†i ƒë√°nh gi√° s·∫£n ph·∫©m sau ƒë√¢y l√† g√¨?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Hi·ªáu nƒÉng tuy·ªát v·ªùi, ƒëi·ªán tho·∫°i ch·∫°y m∆∞·ª£t m√† v√† nhanh ch√≥ng khi s·ª≠ d·ª•ng c√°c ·ª©ng d·ª•ng n·∫∑ng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M·∫´u ƒëi·ªán tho·∫°i m·ªõi n√†y mang l·∫°i hi·ªáu su·∫•t tuy·ªát v·ªùi, nh∆∞ng tu·ªïi th·ªç pin c·ªßa n√≥ c√≥ th·ªÉ t·ªët h∆°n.  ### Instruction:\n",
      "Ch·ªâ ƒë∆∞·ª£c tr·∫£ ra ƒë·∫ßu ra c√¢u h·ªèi v√† kh√¥ng c√≥ gi·∫£i th√≠ch g√¨. D·ªãch c√¢u sau t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát?\n",
      "\n",
      "### Input:\n",
      "\n",
      "The new smartphone model offers excellent performance, but its battery life could be better.\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Ch·ªâ ƒë∆∞·ª£c tr·∫£ ra ƒë·∫ßu ra c√¢u h·ªèi v√† kh√¥ng c√≥ gi·∫£i th√≠ch g√¨. D·ªãch c√¢u sau t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "The new smartphone model offers excellent performance, but its battery life could be better.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√¢u tr·∫£ l·ªùi ƒë√∫ng l√†: trai. B·ªüi v√¨ trong c√¢u chuy·ªán, m·ªçi c·∫≠u b√© ch·ªâ ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠ v√† c√°c c√¥ g√°i kh√¥ng ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠. Do ƒë√≥, ng∆∞·ªùi n√†y ph·∫£i l√† trai.  ### Instruction:\n",
      "Suy lu·∫≠n c√¢u tr·∫£ l·ªùi t·ª´ c√¢u chuy·ªán n√†y?\n",
      "\n",
      "### Input:\n",
      "\n",
      "### C√¢u chuy·ªán:\n",
      "Ng∆∞·ªùi trong l·ªõp n√†y l√† trai ho·∫∑c g√°i. M·ªçi c·∫≠u b√© ch·ªâ ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠ v√† c√°c c√¥ g√°i kh√¥ng ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠. Nh·ªØng ng∆∞·ªùi ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠ ph·∫£i c√≥ m√°y t√≠nh x√°ch tay ch∆°i game.\n",
      "### C√¢u h·ªèi:\n",
      "M·ªôt ng∆∞·ªùi trong l·ªõp n√†y kh√¥ng ch∆°i tr√≤\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Suy lu·∫≠n c√¢u tr·∫£ l·ªùi t·ª´ c√¢u chuy·ªán n√†y?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "### C√¢u chuy·ªán:\n",
    "Ng∆∞·ªùi trong l·ªõp n√†y l√† trai ho·∫∑c g√°i. M·ªçi c·∫≠u b√© ch·ªâ ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠ v√† c√°c c√¥ g√°i kh√¥ng ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠. Nh·ªØng ng∆∞·ªùi ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠ ph·∫£i c√≥ m√°y t√≠nh x√°ch tay ch∆°i game.\n",
    "### C√¢u h·ªèi:\n",
    "M·ªôt ng∆∞·ªùi trong l·ªõp n√†y c√≥ m√°y t√≠nh x√°ch tay ch∆°i game. Ng∆∞·ªùi n√†y l√† trai hay g√°i?\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kepco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
