{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/nmquy/KEPCO/kepco/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 05:18:20.577577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-12 05:18:20.599973: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-12 05:18:20.606693: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-12 05:18:20.623032: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-12 05:18:24.327469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/nmquy/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.586 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir=\"/workspace/nmquy/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table>\n",
      "    <tr>\n",
      "        <th>T√™n</th>\n",
      "        <th>Email</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Shyam</td>\n",
      "        <td>shyamjaiswal@gmail.com</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Bob</td>\n",
      "        <td>bob32@gmail.com</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Jai</td>\n",
      "        <td>jai87@gmail.com</td>\n",
      "    </tr>\n",
      "</table><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Chuy·ªÉn dictionary python sau t·ª´ JSON sang b·∫£ng HTML c√≥ header v√† title c·ªôt. Ch·ªâ s·ª≠ d·ª•ng m√£ HTML:\"\n",
    "input_data = '''\n",
    "{'resturant employees': [{'name': 'Shyam', 'email': 'shyamjaiswal@gmail.com'}, {'name': 'Bob', 'email': 'bob32@gmail.com'}, {'name': 'Jai', 'email': 'jai87@gmail.com'}]}\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot rewrite question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer the question concisely, BERT is a powerful NLP model that can be used for a variety of tasks, including text classification, sentiment analysis, and question answering. Its ability to learn contextual relationships between words makes it particularly well-suited for tasks that require understanding the nuances of language.\n",
      "\n",
      "However, for the sake of completeness, here's a more detailed explanation:\n",
      "\n",
      "### Detailed Explanation:\n",
      "BERT has been shown to perform well on a range of NLP tasks, including:\n",
      "\n",
      "*   Text classification: BERT can be used to classify text into different categories, such as spam vs. non-spam emails or positive vs. negative product reviews.\n",
      "*   Sentiment analysis: BERT can be used to determine the sentiment of text, such as whether a customer review is positive or negative.\n",
      "*   Question answering: BERT can be used to answer questions based on the input text, such as determining the definition of a word or identifying the main topic of a passage.\n",
      "*   Language translation: BERT can be used to translate text from one language to another.\n",
      "*   Named entity recognition: BERT can be used to identify named entities in text, such as people, places, and organizations.\n",
      "\n",
      "Overall, BERT is a versatile model that can be used for a\n"
     ]
    }
   ],
   "source": [
    "instruction = \"You are able to reason from previous conversation and the recent question, to come up with a rewrite of the question which is concise but with enough information that people without knowledge of previous conversation can understand the question:\"\n",
    "input_data = '''\n",
    "## Previous conversation\n",
    "user: What is BERT?\n",
    "assistant: BERT stands for \"Bidirectional Encoder Representations from Transformers.\" It is a natural language processing (NLP) model developed by Google. \n",
    "user: What data was used for its training?\n",
    "assistant: The BERT (Bidirectional Encoder Representations from Transformers) model was trained on a large corpus of publicly available text from the internet. It was trained on a combination of books, articles, websites, and other sources to learn the language patterns and relationships between words.\n",
    "## Question\n",
    "user: What NLP tasks can it perform well?\n",
    "## Rewritten question\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B∆∞·ªõc 1 - L·∫•y m·ªôt s·ªë n∆∞·ªõc s√¥i.\n",
      "B∆∞·ªõc 2 - L·∫•y m·ªôt chi·∫øc c·ªëc v√† ƒë·∫∑t m·ªôt t√∫i tr√† v√†o ƒë√≥.\n",
      "B∆∞·ªõc 3 - ƒê·ªï n∆∞·ªõc s√¥i l√™n t√∫i tr√†.\n",
      "B∆∞·ªõc 4 - ƒê·ªÉ t√∫i tr√† ng·ªìi m·ªôt ch√∫t ƒë·ªÉ tr√† c√≥ th·ªÉ ng√¢m.\n",
      "B∆∞·ªõc 5 - L·∫•y t√∫i tr√† ra.\n",
      "B∆∞·ªõc 6 - Th√™m m·ªôt √≠t ƒë∆∞·ªùng ho·∫∑c s·ªØa cho v·ª´a ƒÉn (n·∫øu mu·ªën).\n",
      "B∆∞·ªõc 7 - Th∆∞·ªüng th·ª©c t√°ch tr√† ƒë√£ s·∫µn s√†ng!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p vƒÉn b·∫£n. N·∫øu n√≥ ch·ª©a m·ªôt chu·ªói c√°c h∆∞·ªõng d·∫´n, vi·∫øt l·∫°i c√°c h∆∞·ªõng d·∫´n ƒë√≥ theo ƒë·ªãnh d·∫°ng sau:\n",
    "B∆∞·ªõc 1 - ...\n",
    "B∆∞·ªõc 2 -‚Ä¶\n",
    "‚Ä¶\n",
    "B∆∞·ªõc N -‚Ä¶\n",
    "\n",
    "N·∫øu vƒÉn b·∫£n kh√¥ng ch·ª©a m·ªôt chu·ªói h∆∞·ªõng d·∫´n, sau ƒë√≥ ch·ªâ c·∫ßn vi·∫øt \"No steps provided.\\\"\"\"\"\n",
    "input_data = '''\n",
    "L√†m m·ªôt t√°ch tr√† th·∫≠t d·ªÖ d√†ng! ƒê·∫ßu ti√™n, b·∫°n c·∫ßn l·∫•y m·ªôt s·ªë  n∆∞·ªõc s√¥i. Trong khi ƒëi·ªÅu ƒë√≥ ƒëang x·∫£y ra,  \n",
    "l·∫•y m·ªôt chi·∫øc c·ªëc v√† ƒë·∫∑t m·ªôt t√∫i tr√† v√†o ƒë√≥. M·ªôt khi n∆∞·ªõc ƒë√£ ƒë·ªß n√≥ng, ch·ªâ c·∫ßn ƒë·ªï n√≥ l√™n t√∫i tr√†.  \n",
    "H√£y ƒë·ªÉ n√≥ ng·ªìi m·ªôt ch√∫t ƒë·ªÉ tr√† c√≥ th·ªÉ ng√¢m. Sau m·ªôt v√†i ph√∫t, l·∫•y t√∫i tr√† ra. N·∫øu b·∫°n  \n",
    "nh∆∞, b·∫°n c√≥ th·ªÉ th√™m m·ªôt √≠t ƒë∆∞·ªùng ho·∫∑c s·ªØa cho v·ª´a ƒÉn. V√† th·∫ø l√† xong! B·∫°n ƒë√£ c√≥ cho m√¨nh m·ªôt m√≥n ngon t√°ch tr√† ƒë·ªÉ th∆∞·ªüng th·ª©c.\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow many instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. T√≥m t·∫Øt ƒëo·∫°n vƒÉn b·∫£n sau.\n",
      "\n",
      "V·ª• ch√°y nh√† kho ch·ª©a ƒë·ªì g·ªó C√¥ng ty TNHH V∆∞·ª£ng H√† ·ªü x√£ C∆∞∆°ng S∆°n, huy·ªán L·ª•c Nam, ƒë√£ g√¢y thi·ªát h·∫°i v·ªÅ t√†i s·∫£n. Kho·∫£ng 15 ph√∫t sau khi ng·ªçn l·ª≠a b√πng l√™n, n√≥ ƒë√£ lan ra to√†n b·ªô nh√† x∆∞·ªüng v√† c·ªôt c√≥i, g√¢y thi·ªát h·∫°i nghi√™m tr·ªçng.\n",
      "\n",
      "2. D·ªãch t√≥m t·∫Øt sang ti·∫øng Anh.\n",
      "\n",
      "A fire at a warehouse containing timber in V∆∞·ª£ng H√† Company, in C∆∞∆°ng S∆°n village, L·ª•c Nam district, caused significant property damage. The fire broke out around 15 minutes ago and quickly spread throughout the warehouse and the roof, causing extensive damage.\n",
      "\n",
      "3. Xu·∫•t ra m·ªôt ƒë·ªëi t∆∞·ª£ng json ch·ª©a c√°c kh√≥a sau: Vietnamese_summary, English_summary.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Vietnamese_summary\": \"V·ª• ch√°y nh√† kho ch·ª©a ƒë·ªì g·ªó C√¥ng ty TNHH V∆∞·ª£ng H√† ·ªü x√£ C∆∞∆°ng S∆°n, huy·ªán L·ª•c Nam, ƒë√£ g√¢y thi·ªát h·∫°i v·ªÅ t√†i s·∫£n. Kho·∫£ng 15 ph√∫t sau khi ng·ªçn l·ª≠a b√πng l√™n, n√≥ ƒë√£ lan ra to√†n b·ªô nh√† x∆∞·ªüng v√† c·ªôt c√≥i, g√¢y thi·ªát h·∫°i nghi√™m tr·ªçng.\",\n",
      "  \"English_summary\": \"A fire at a warehouse containing timber in V∆∞·ª£ng H√† Company, in C∆∞∆°ng S∆°n village, L·ª•c Nam district, caused significant property damage. The fire broke out around 15 minutes ago and quickly spread throughout the warehouse and the roof, causing extensive damage.\"\n",
      "}\n",
      "```<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Th·ª±c hi·ªán c√°c h√†nh ƒë·ªông sau: \n",
    "1 - T√≥m t·∫Øt ƒëo·∫°n vƒÉn b·∫£n sau.\n",
    "2 - D·ªãch t√≥m t·∫Øt sang ti·∫øng Anh.\n",
    "3 - Xu·∫•t ra m·ªôt ƒë·ªëi t∆∞·ª£ng json ch·ª©a c√°c kh√≥a sau: Vietnamese_summary, English_summary.\n",
    "T√°ch c√°c c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n b·∫±ng d·∫•u ng·∫Øt d√≤ng.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Kho·∫£ng 15h, nh√† kho ch·ª©a ƒë·ªì g·ªó C√¥ng ty TNHH V∆∞·ª£ng H√† ·ªü x√£ C∆∞∆°ng S∆°n, huy·ªán L·ª•c Nam b·ªëc ch√°y. M·ªôt ng∆∞·ªùi ƒë√†n √¥ng t·ª´ b√™n trong ch·∫°y ra ngo√†i h√¥ ho√°n nh·ªù h·ªó tr·ª£.\n",
    "\n",
    "Tuy nhi√™n do v·∫≠t li·ªáu d·ªÖ ch√°y n√™n ch·ªâ trong √≠t ph√∫t, ng·ªçn l·ª≠a ƒë√£ b√πng l√™n b·∫•t ch·∫•p n·ªó l·ª±c ch·ªØa ch√°y c·ªßa ng∆∞·ªùi d√¢n. Kho·∫£ng 15 ph√∫t sau, ng·ªçn l·ª≠a ƒë√£ lan ra to√†n b·ªô nh√† x∆∞·ªüng, c·ªôt c√≥i nghi ng√∫t b·ªëc cao h√†ng ch·ª•c m√©t.\n",
    "Ph√≤ng C·∫£nh s√°t ph√≤ng ch√°y, ch·ªØa ch√°y v√† c·ª©u n·∫°n c·ª©u h·ªô, C√¥ng an huy·ªán L·ª•c Nam ƒë√£ huy ƒë·ªông 40 chi·∫øn sƒ© c√πng ba xe ch·ªØa ch√°y ƒë·∫øn hi·ªán tr∆∞·ªùng. Sau kho·∫£ng 40 ph√∫t, ng·ªçn l·ª≠a ƒë∆∞·ª£c d·∫≠p t·∫Øt nh∆∞ng h·∫ßu h·∫øt t√†i s·∫£n b√™n trong b·ªã thi√™u r·ª•i.\n",
    "\n",
    "√îng Nguy·ªÖn M·∫°nh H∆∞ng, Ch·ªß t·ªãch UBND x√£ C∆∞∆°ng S∆°n, cho bi·∫øt v·ª• ch√°y kh√¥ng g√¢y thi·ªát h·∫°i v·ªÅ ng∆∞·ªùi. Tuy nhi√™n, x∆∞·ªüng g·ªó r·ªông kho·∫£ng 400 m2 n·∫±m s√°t ƒë∆∞·ªùng t·ªânh 293 ƒë√£ b·ªã thi√™u r·ª•i. To√†n b·ªô ph·∫ßn m√°i t√¥n v√† c·∫•u ki·ªán b√™n trong s·∫≠p ƒë·ªï, t∆∞·ªùng t√¥n b·ªã u·ªën cong, bi·∫øn d·∫°ng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông l√† 515y+50.000 USD, v·ªõi y l√† di·ªán t√≠ch c·ªßa t√≤a nh√†. \n",
      "\n",
      "**L·ªùi gi·∫£i th√≠ch:** Trong tr∆∞·ªùng h·ª£p n√†y, c√°ch gi·∫£i quy·∫øt c·ªßa h·ªçc sinh ƒë√£ x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c chi ph√≠ li√™n quan ƒë·∫øn vi·ªác x√¢y d·ª±ng v√† b·∫£o tr√¨ t√≤a nh√†. T·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ƒë∆∞·ª£c t√≠nh b·∫±ng c√°ch th√™m c√°c chi ph√≠ ƒë·∫•t, v·∫≠t li·ªáu x√¢y d·ª±ng v√† b·∫£o tr√¨. S·ª± k·∫øt h·ª£p c·ªßa c√°c y·∫øu t·ªë n√†y ƒë√£ t·∫°o ra m·ªôt h√†m t·ªïng chi ph√≠ c√≥ th·ªÉ t√≠nh ƒë∆∞·ª£c d·ª±a tr√™n di·ªán t√≠ch c·ªßa t√≤a nh√†. \n",
      "\n",
      "**V·∫•n ƒë·ªÅ:** ƒê·ªÉ x√°c ƒë·ªãnh xem c√°ch gi·∫£i quy·∫øt c·ªßa h·ªçc sinh c√≥ ƒë√∫ng hay kh√¥ng, ch√∫ng ta c·∫ßn ki·ªÉm tra t√≠nh ch√≠nh x√°c c·ªßa ph∆∞∆°ng tr√¨nh t·ªïng chi ph√≠ v√† t√≠nh to√°n chi ph√≠ cho m·ªôt di·ªán t√≠ch c·ª• th·ªÉ ƒë·ªÉ x√°c ƒë·ªãnh xem c√≥ ph·∫£i l√† m·ªôt ph∆∞∆°ng tr√¨nh ch√≠nh x√°c hay kh√¥ng.\n",
      "\n",
      "**Th√†nh ph·∫ßn c·ªßa ph∆∞∆°ng tr√¨nh:** Ph∆∞∆°ng tr√¨nh ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ c√°c chi ph√≠ c∆° b·∫£n nh∆∞ gi√° ƒë·∫•t, gi√° v·∫≠t li·ªáu x√¢y d·ª±ng v√† chi ph√≠ b·∫£o tr√¨. C√°c chi ph√≠ n√†y ƒë√£ ƒë∆∞·ª£c k·∫øt h·ª£p ƒë·ªÉ t·∫°o ra m·ªôt t·ªïng chi ph√≠ c√≥ th·ªÉ t√≠nh ƒë∆∞·ª£c d·ª±a tr√™n di·ªán t√≠ch c·ªßa t√≤a nh√†.\n",
      "\n",
      "**S·ª± ch√≠nh x√°c:** ƒê·ªÉ x√°c ƒë·ªãnh xem ph∆∞∆°ng tr√¨nh c√≥ ch√≠nh x√°c hay kh√¥ng, ch√∫ng ta c·∫ßn ki·ªÉm tra c√°c y·∫øu t·ªë nh∆∞ gi√° ƒë·∫•t, gi√° v·∫≠t li·ªáu x√¢y d·ª±ng v√† chi ph√≠ b·∫£o tr√¨. N·∫øu ch√∫ng ta c√≥ th√¥ng tin ch√≠nh x√°c v·ªÅ gi√° ƒë·∫•t v√† v·∫≠t li·ªáu x√¢y d·ª±ng, v√† chi ph√≠ b·∫£o tr√¨ ƒë∆∞·ª£c t√≠nh to√°n ch√≠nh x√°c, th√¨ ph∆∞∆°ng tr√¨nh c√≥ th·ªÉ ƒë∆∞·ª£c xem l√† ch√≠nh x√°c. Tuy nhi√™n, n·∫øu c√≥ b·∫•t k·ª≥ sai s√≥t n√†o trong c√°c y·∫øu t·ªë n√†y, ph∆∞∆°ng tr√¨nh c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c.\n",
      "\n",
      "**V√≠ d·ª•:** ƒê·ªÉ ki·ªÉm tra t√≠nh ch√≠nh x√°c c·ªßa ph∆∞∆°ng tr√¨nh, ch√∫ng ta c√≥ th·ªÉ t√≠nh to√°n t·ªïng chi ph√≠ cho m·ªôt di·ªán t√≠ch c·ª• th·ªÉ, ch·∫≥ng h·∫°n nh∆∞ 1000 m√©t vu√¥ng. Sau ƒë√≥, ch√∫ng ta c√≥ th·ªÉ so s√°nh t·ªïng chi ph√≠ t√≠nh to√°n v·ªõi t·ªïng chi ph√≠ th·ª±c t·∫ø ƒë·ªÉ x√°c ƒë·ªãnh xem ph∆∞∆°ng tr√¨nh c√≥ ch√≠nh x√°c hay kh√¥ng.\n",
      "\n",
      "**K·∫øt lu·∫≠n:** T·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông l√† 515y+50.000 USD, v·ªõi y l√† di·ªán t√≠ch c·ªßa t√≤a nh√†. Ph∆∞∆°ng tr√¨nh n√†y ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ c√°c chi ph√≠ c∆° b·∫£n v√† ƒë√£ ƒë∆∞·ª£c k·∫øt h·ª£p ƒë·ªÉ t·∫°o ra m·ªôt t·ªïng chi ph√≠ c√≥ th·ªÉ t√≠nh ƒë∆∞·ª£c d·ª±a tr√™n di·ªán t√≠ch c·ªßa t√≤a nh√†. Tuy nhi√™n, ƒë·ªÉ x√°c ƒë·ªãnh xem ph∆∞∆°ng tr√¨nh c√≥ ch√≠nh x√°c hay kh√¥ng, ch√∫ng ta c·∫ßn ki·ªÉm tra c√°c y·∫øu t·ªë\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"X√°c ƒë·ªãnh xem c√°ch gi·∫£i quy·∫øt c·ªßa h·ªçc sinh c√≥ ƒë√∫ng hay kh√¥ng.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "C√¢u h·ªèi: T√¥i ƒëang l√™n k·∫ø ho·∫°ch x√¢y d·ª±ng m·ªôt t√≤a nh√† vƒÉn ph√≤ng v√† c·∫ßn t√≠nh to√°n chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông.\n",
    "\n",
    "Gi√° ƒë·∫•t l√† 200 USD/m2.\n",
    "T√¥i c√≥ th·ªÉ mua v·∫≠t li·ªáu x√¢y d·ª±ng v·ªõi gi√° 300 USD/m2.\n",
    "Chi ph√≠ b·∫£o tr√¨ h·∫±ng nƒÉm l√† 50 ngh√¨n USD cho to√†n b·ªô t√≤a nh√† v√† th√™m 15 USD/m2. T·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông l√† bao nhi√™u, nh∆∞ m·ªôt h√†m c·ªßa di·ªán t√≠ch t√≤a nh√† t√≠nh b·∫±ng m√©t vu√¥ng.\n",
    "Gi·∫£i ph√°p c·ªßa h·ªçc sinh: G·ªçi y l√† di·ªán t√≠ch c·ªßa t√≤a nh√† t√≠nh b·∫±ng m√©t vu√¥ng.\n",
    "\n",
    "Chi ph√≠:\n",
    "Gi√° ƒë·∫•t: 200y USD\n",
    "Gi√° v·∫≠t li·ªáu x√¢y d·ª±ng: 300y USD\n",
    "Chi ph√≠ b·∫£o tr√¨: 50.000+15y USD\n",
    "T·ªïng chi ph√≠: \n",
    "200y+300y+50.000+15y=515y+50.000 (USD)\n",
    "T·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n s·∫Ω l√† : 515y+50.000 USD, v·ªõi y l√† di·ªán t√≠ch c·ªßa t√≤a nh√†.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√°nh gi√° ƒëi·ªán tho·∫°i n√†y cho th·∫•y hi·ªáu nƒÉng m·∫°nh m·∫Ω, th·ªùi l∆∞·ª£ng pin ·∫•n t∆∞·ª£ng v√† m√†n h√¨nh s·∫Øc n√©t. M·∫∑c d√π c√≥ m·ªôt s·ªë h·∫°n ch·∫ø nh∆∞ camera v√† loa, gi√° h·ª£p l√Ω khi·∫øn n√≥ ƒë∆∞·ª£c ƒë√°nh gi√° cao. (30 t·ª´)<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Nhi·ªám v·ª• c·ªßa b·∫°n l√† t·∫°o m·ªôt b·∫£n t√≥m t·∫Øt ng·∫Øn v·ªÅ ƒë√°nh gi√° s·∫£n ph·∫©m t·ª´ m·ªôt trang web th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. \n",
    "T√≥m t·∫Øt ƒë√°nh gi√° d∆∞·ªõi ƒë√¢y t·ªëi ƒëa 30 t·ª´.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Chi·∫øc ƒëi·ªán tho·∫°i th√¥ng minh nh·∫≠n ƒë∆∞·ª£c nhi·ªÅu ƒë√°nh gi√° t√≠ch c·ª±c t·ª´ kh√°ch h√†ng tr√™n trang web th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. H·ªç khen ng·ª£i hi·ªáu nƒÉng m·∫°nh m·∫Ω, t·ªëc ƒë·ªô x·ª≠ l√Ω nhanh v√† m∆∞·ª£t m√†, c√πng v·ªõi th·ªùi l∆∞·ª£ng pin ·∫•n t∆∞·ª£ng, c√≥ th·ªÉ s·ª≠ d·ª•ng c·∫£ ng√†y m√† kh√¥ng c·∫ßn s·∫°c. M√†n h√¨nh s·∫Øc n√©t v√† t√≠nh nƒÉng b·∫£o m·∫≠t v√¢n tay ti·ªán l·ª£i c≈©ng l√† nh·ªØng ƒëi·ªÉm c·ªông. Tuy nhi√™n, m·ªôt s·ªë kh√°ch h√†ng cho r·∫±ng camera ch∆∞a th·ª±c s·ª± t·ªët trong ƒëi·ªÅu ki·ªán thi·∫øu s√°ng, thi·∫øt k·∫ø c·ªßa ƒëi·ªán tho·∫°i h∆°i c·ªìng k·ªÅnh, v√† loa ngo√†i kh√¥ng ƒë·ªß l·ªõn. V·ªõi m·ª©c gi√° h·ª£p l√Ω, s·∫£n ph·∫©m n√†y v·∫´n ƒë∆∞·ª£c ƒë√°nh gi√° cao v·ªÅ ch·∫•t l∆∞·ª£ng t·ªïng th·ªÉ.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C·∫£m nh·∫≠n c·ªßa b√†i ƒë√°nh gi√° s·∫£n ph·∫©m n√†y l√† r·∫•t t√≠ch c·ª±c. Ng∆∞·ªùi vi·∫øt ƒë√°nh gi√° cho th·∫•y ƒëi·ªán tho·∫°i c·ªßa h·ªç ho·∫°t ƒë·ªông r·∫•t hi·ªáu qu·∫£, ch·∫°y m∆∞·ª£t m√† v√† nhanh ch√≥ng khi s·ª≠ d·ª•ng c√°c ·ª©ng d·ª•ng n·∫∑ng. ƒêi·ªÅu n√†y cho th·∫•y h·ªç r·∫•t h√†i l√≤ng v·ªõi hi·ªáu nƒÉng c·ªßa s·∫£n ph·∫©m. \n",
      "\n",
      "Tuy nhi√™n, c·∫ßn l∆∞u √Ω r·∫±ng b√†i ƒë√°nh gi√° n√†y kh√¥ng cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ s·∫£n ph·∫©m, ch·ªâ t·∫≠p trung v√†o hi·ªáu nƒÉng c·ªßa n√≥. ƒê·ªÉ c√≥ m·ªôt c√°i nh√¨n to√†n di·ªán v·ªÅ s·∫£n ph·∫©m, c·∫ßn ph·∫£i xem x√©t nhi·ªÅu y·∫øu t·ªë kh√°c nh∆∞ thi·∫øt k·∫ø, gi√° c·∫£, v√† ch·∫•t l∆∞·ª£ng camera. \n",
      "\n",
      "Tuy nhi√™n,\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"C·∫£m nh·∫≠n c·ªßa b√†i ƒë√°nh gi√° s·∫£n ph·∫©m sau ƒë√¢y l√† g√¨?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Hi·ªáu nƒÉng tuy·ªát v·ªùi, ƒëi·ªán tho·∫°i ch·∫°y m∆∞·ª£t m√† v√† nhanh ch√≥ng khi s·ª≠ d·ª•ng c√°c ·ª©ng d·ª•ng n·∫∑ng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêi·ªán tho·∫°i th√¥ng minh m·ªõi n√†y cung c·∫•p hi·ªáu su·∫•t tuy·ªát v·ªùi, nh∆∞ng th·ªùi l∆∞·ª£ng pin c·ªßa n√≥ c√≥ th·ªÉ t·ªët h∆°n.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Ch·ªâ ƒë∆∞·ª£c tr·∫£ ra ƒë·∫ßu ra c√¢u h·ªèi v√† kh√¥ng c√≥ gi·∫£i th√≠ch g√¨. D·ªãch c√¢u sau t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "The new smartphone model offers excellent performance, but its battery life could be better.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ng∆∞·ªùi n√†y ph·∫£i l√† g√°i. V√¨ m·ªçi c√¥ g√°i kh√¥ng ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠, ng∆∞·ªùi n√†y ph·∫£i l√† g√°i.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Suy lu·∫≠n c√¢u tr·∫£ l·ªùi t·ª´ c√¢u chuy·ªán n√†y?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "### C√¢u chuy·ªán:\n",
    "Ng∆∞·ªùi trong l·ªõp n√†y l√† trai ho·∫∑c g√°i. M·ªçi c·∫≠u b√© ch·ªâ ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠ v√† c√°c c√¥ g√°i kh√¥ng ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠. Nh·ªØng ng∆∞·ªùi ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠ ph·∫£i c√≥ m√°y t√≠nh x√°ch tay ch∆°i game.\n",
    "### C√¢u h·ªèi:\n",
    "M·ªôt ng∆∞·ªùi trong l·ªõp n√†y c√≥ m√°y t√≠nh x√°ch tay ch∆°i game. Ng∆∞·ªùi n√†y l√† trai hay g√°i?\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kepco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
