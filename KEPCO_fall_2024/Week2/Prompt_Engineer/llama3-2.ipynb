{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/nmquy/KEPCO/kepco/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 05:18:20.577577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-12 05:18:20.599973: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-12 05:18:20.606693: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-12 05:18:20.623032: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-12 05:18:24.327469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/nmquy/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.586 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir=\"/workspace/nmquy/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table>\n",
      "    <tr>\n",
      "        <th>Tên</th>\n",
      "        <th>Email</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Shyam</td>\n",
      "        <td>shyamjaiswal@gmail.com</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Bob</td>\n",
      "        <td>bob32@gmail.com</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Jai</td>\n",
      "        <td>jai87@gmail.com</td>\n",
      "    </tr>\n",
      "</table><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Chuyển dictionary python sau từ JSON sang bảng HTML có header và title cột. Chỉ sử dụng mã HTML:\"\n",
    "input_data = '''\n",
    "{'resturant employees': [{'name': 'Shyam', 'email': 'shyamjaiswal@gmail.com'}, {'name': 'Bob', 'email': 'bob32@gmail.com'}, {'name': 'Jai', 'email': 'jai87@gmail.com'}]}\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot rewrite question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer the question concisely, BERT is a powerful NLP model that can be used for a variety of tasks, including text classification, sentiment analysis, and question answering. Its ability to learn contextual relationships between words makes it particularly well-suited for tasks that require understanding the nuances of language.\n",
      "\n",
      "However, for the sake of completeness, here's a more detailed explanation:\n",
      "\n",
      "### Detailed Explanation:\n",
      "BERT has been shown to perform well on a range of NLP tasks, including:\n",
      "\n",
      "*   Text classification: BERT can be used to classify text into different categories, such as spam vs. non-spam emails or positive vs. negative product reviews.\n",
      "*   Sentiment analysis: BERT can be used to determine the sentiment of text, such as whether a customer review is positive or negative.\n",
      "*   Question answering: BERT can be used to answer questions based on the input text, such as determining the definition of a word or identifying the main topic of a passage.\n",
      "*   Language translation: BERT can be used to translate text from one language to another.\n",
      "*   Named entity recognition: BERT can be used to identify named entities in text, such as people, places, and organizations.\n",
      "\n",
      "Overall, BERT is a versatile model that can be used for a\n"
     ]
    }
   ],
   "source": [
    "instruction = \"You are able to reason from previous conversation and the recent question, to come up with a rewrite of the question which is concise but with enough information that people without knowledge of previous conversation can understand the question:\"\n",
    "input_data = '''\n",
    "## Previous conversation\n",
    "user: What is BERT?\n",
    "assistant: BERT stands for \"Bidirectional Encoder Representations from Transformers.\" It is a natural language processing (NLP) model developed by Google. \n",
    "user: What data was used for its training?\n",
    "assistant: The BERT (Bidirectional Encoder Representations from Transformers) model was trained on a large corpus of publicly available text from the internet. It was trained on a combination of books, articles, websites, and other sources to learn the language patterns and relationships between words.\n",
    "## Question\n",
    "user: What NLP tasks can it perform well?\n",
    "## Rewritten question\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bước 1 - Lấy một số nước sôi.\n",
      "Bước 2 - Lấy một chiếc cốc và đặt một túi trà vào đó.\n",
      "Bước 3 - Đổ nước sôi lên túi trà.\n",
      "Bước 4 - Để túi trà ngồi một chút để trà có thể ngâm.\n",
      "Bước 5 - Lấy túi trà ra.\n",
      "Bước 6 - Thêm một ít đường hoặc sữa cho vừa ăn (nếu muốn).\n",
      "Bước 7 - Thưởng thức tách trà đã sẵn sàng!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Bạn sẽ được cung cấp văn bản. Nếu nó chứa một chuỗi các hướng dẫn, viết lại các hướng dẫn đó theo định dạng sau:\n",
    "Bước 1 - ...\n",
    "Bước 2 -…\n",
    "…\n",
    "Bước N -…\n",
    "\n",
    "Nếu văn bản không chứa một chuỗi hướng dẫn, sau đó chỉ cần viết \"No steps provided.\\\"\"\"\"\n",
    "input_data = '''\n",
    "Làm một tách trà thật dễ dàng! Đầu tiên, bạn cần lấy một số  nước sôi. Trong khi điều đó đang xảy ra,  \n",
    "lấy một chiếc cốc và đặt một túi trà vào đó. Một khi nước đã đủ nóng, chỉ cần đổ nó lên túi trà.  \n",
    "Hãy để nó ngồi một chút để trà có thể ngâm. Sau một vài phút, lấy túi trà ra. Nếu bạn  \n",
    "như, bạn có thể thêm một ít đường hoặc sữa cho vừa ăn. Và thế là xong! Bạn đã có cho mình một món ngon tách trà để thưởng thức.\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow many instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tóm tắt đoạn văn bản sau.\n",
      "\n",
      "Vụ cháy nhà kho chứa đồ gỗ Công ty TNHH Vượng Hà ở xã Cương Sơn, huyện Lục Nam, đã gây thiệt hại về tài sản. Khoảng 15 phút sau khi ngọn lửa bùng lên, nó đã lan ra toàn bộ nhà xưởng và cột cói, gây thiệt hại nghiêm trọng.\n",
      "\n",
      "2. Dịch tóm tắt sang tiếng Anh.\n",
      "\n",
      "A fire at a warehouse containing timber in Vượng Hà Company, in Cương Sơn village, Lục Nam district, caused significant property damage. The fire broke out around 15 minutes ago and quickly spread throughout the warehouse and the roof, causing extensive damage.\n",
      "\n",
      "3. Xuất ra một đối tượng json chứa các khóa sau: Vietnamese_summary, English_summary.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Vietnamese_summary\": \"Vụ cháy nhà kho chứa đồ gỗ Công ty TNHH Vượng Hà ở xã Cương Sơn, huyện Lục Nam, đã gây thiệt hại về tài sản. Khoảng 15 phút sau khi ngọn lửa bùng lên, nó đã lan ra toàn bộ nhà xưởng và cột cói, gây thiệt hại nghiêm trọng.\",\n",
      "  \"English_summary\": \"A fire at a warehouse containing timber in Vượng Hà Company, in Cương Sơn village, Lục Nam district, caused significant property damage. The fire broke out around 15 minutes ago and quickly spread throughout the warehouse and the roof, causing extensive damage.\"\n",
      "}\n",
      "```<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Thực hiện các hành động sau: \n",
    "1 - Tóm tắt đoạn văn bản sau.\n",
    "2 - Dịch tóm tắt sang tiếng Anh.\n",
    "3 - Xuất ra một đối tượng json chứa các khóa sau: Vietnamese_summary, English_summary.\n",
    "Tách các câu trả lời của bạn bằng dấu ngắt dòng.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Khoảng 15h, nhà kho chứa đồ gỗ Công ty TNHH Vượng Hà ở xã Cương Sơn, huyện Lục Nam bốc cháy. Một người đàn ông từ bên trong chạy ra ngoài hô hoán nhờ hỗ trợ.\n",
    "\n",
    "Tuy nhiên do vật liệu dễ cháy nên chỉ trong ít phút, ngọn lửa đã bùng lên bất chấp nỗ lực chữa cháy của người dân. Khoảng 15 phút sau, ngọn lửa đã lan ra toàn bộ nhà xưởng, cột cói nghi ngút bốc cao hàng chục mét.\n",
    "Phòng Cảnh sát phòng cháy, chữa cháy và cứu nạn cứu hộ, Công an huyện Lục Nam đã huy động 40 chiến sĩ cùng ba xe chữa cháy đến hiện trường. Sau khoảng 40 phút, ngọn lửa được dập tắt nhưng hầu hết tài sản bên trong bị thiêu rụi.\n",
    "\n",
    "Ông Nguyễn Mạnh Hưng, Chủ tịch UBND xã Cương Sơn, cho biết vụ cháy không gây thiệt hại về người. Tuy nhiên, xưởng gỗ rộng khoảng 400 m2 nằm sát đường tỉnh 293 đã bị thiêu rụi. Toàn bộ phần mái tôn và cấu kiện bên trong sập đổ, tường tôn bị uốn cong, biến dạng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng chi phí cho năm đầu tiên hoạt động là 515y+50.000 USD, với y là diện tích của tòa nhà. \n",
      "\n",
      "**Lời giải thích:** Trong trường hợp này, cách giải quyết của học sinh đã xác định được các chi phí liên quan đến việc xây dựng và bảo trì tòa nhà. Tổng chi phí cho năm đầu tiên được tính bằng cách thêm các chi phí đất, vật liệu xây dựng và bảo trì. Sự kết hợp của các yếu tố này đã tạo ra một hàm tổng chi phí có thể tính được dựa trên diện tích của tòa nhà. \n",
      "\n",
      "**Vấn đề:** Để xác định xem cách giải quyết của học sinh có đúng hay không, chúng ta cần kiểm tra tính chính xác của phương trình tổng chi phí và tính toán chi phí cho một diện tích cụ thể để xác định xem có phải là một phương trình chính xác hay không.\n",
      "\n",
      "**Thành phần của phương trình:** Phương trình được xây dựng từ các chi phí cơ bản như giá đất, giá vật liệu xây dựng và chi phí bảo trì. Các chi phí này đã được kết hợp để tạo ra một tổng chi phí có thể tính được dựa trên diện tích của tòa nhà.\n",
      "\n",
      "**Sự chính xác:** Để xác định xem phương trình có chính xác hay không, chúng ta cần kiểm tra các yếu tố như giá đất, giá vật liệu xây dựng và chi phí bảo trì. Nếu chúng ta có thông tin chính xác về giá đất và vật liệu xây dựng, và chi phí bảo trì được tính toán chính xác, thì phương trình có thể được xem là chính xác. Tuy nhiên, nếu có bất kỳ sai sót nào trong các yếu tố này, phương trình có thể không chính xác.\n",
      "\n",
      "**Ví dụ:** Để kiểm tra tính chính xác của phương trình, chúng ta có thể tính toán tổng chi phí cho một diện tích cụ thể, chẳng hạn như 1000 mét vuông. Sau đó, chúng ta có thể so sánh tổng chi phí tính toán với tổng chi phí thực tế để xác định xem phương trình có chính xác hay không.\n",
      "\n",
      "**Kết luận:** Tổng chi phí cho năm đầu tiên hoạt động là 515y+50.000 USD, với y là diện tích của tòa nhà. Phương trình này đã được xây dựng từ các chi phí cơ bản và đã được kết hợp để tạo ra một tổng chi phí có thể tính được dựa trên diện tích của tòa nhà. Tuy nhiên, để xác định xem phương trình có chính xác hay không, chúng ta cần kiểm tra các yếu tố\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Xác định xem cách giải quyết của học sinh có đúng hay không.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Câu hỏi: Tôi đang lên kế hoạch xây dựng một tòa nhà văn phòng và cần tính toán chi phí cho năm đầu tiên hoạt động.\n",
    "\n",
    "Giá đất là 200 USD/m2.\n",
    "Tôi có thể mua vật liệu xây dựng với giá 300 USD/m2.\n",
    "Chi phí bảo trì hằng năm là 50 nghìn USD cho toàn bộ tòa nhà và thêm 15 USD/m2. Tổng chi phí cho năm đầu tiên hoạt động là bao nhiêu, như một hàm của diện tích tòa nhà tính bằng mét vuông.\n",
    "Giải pháp của học sinh: Gọi y là diện tích của tòa nhà tính bằng mét vuông.\n",
    "\n",
    "Chi phí:\n",
    "Giá đất: 200y USD\n",
    "Giá vật liệu xây dựng: 300y USD\n",
    "Chi phí bảo trì: 50.000+15y USD\n",
    "Tổng chi phí: \n",
    "200y+300y+50.000+15y=515y+50.000 (USD)\n",
    "Tổng chi phí cho năm đầu tiên sẽ là : 515y+50.000 USD, với y là diện tích của tòa nhà.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đánh giá điện thoại này cho thấy hiệu năng mạnh mẽ, thời lượng pin ấn tượng và màn hình sắc nét. Mặc dù có một số hạn chế như camera và loa, giá hợp lý khiến nó được đánh giá cao. (30 từ)<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Nhiệm vụ của bạn là tạo một bản tóm tắt ngắn về đánh giá sản phẩm từ một trang web thương mại điện tử. \n",
    "Tóm tắt đánh giá dưới đây tối đa 30 từ.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Chiếc điện thoại thông minh nhận được nhiều đánh giá tích cực từ khách hàng trên trang web thương mại điện tử. Họ khen ngợi hiệu năng mạnh mẽ, tốc độ xử lý nhanh và mượt mà, cùng với thời lượng pin ấn tượng, có thể sử dụng cả ngày mà không cần sạc. Màn hình sắc nét và tính năng bảo mật vân tay tiện lợi cũng là những điểm cộng. Tuy nhiên, một số khách hàng cho rằng camera chưa thực sự tốt trong điều kiện thiếu sáng, thiết kế của điện thoại hơi cồng kềnh, và loa ngoài không đủ lớn. Với mức giá hợp lý, sản phẩm này vẫn được đánh giá cao về chất lượng tổng thể.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cảm nhận của bài đánh giá sản phẩm này là rất tích cực. Người viết đánh giá cho thấy điện thoại của họ hoạt động rất hiệu quả, chạy mượt mà và nhanh chóng khi sử dụng các ứng dụng nặng. Điều này cho thấy họ rất hài lòng với hiệu năng của sản phẩm. \n",
      "\n",
      "Tuy nhiên, cần lưu ý rằng bài đánh giá này không cung cấp thông tin chi tiết về sản phẩm, chỉ tập trung vào hiệu năng của nó. Để có một cái nhìn toàn diện về sản phẩm, cần phải xem xét nhiều yếu tố khác như thiết kế, giá cả, và chất lượng camera. \n",
      "\n",
      "Tuy nhiên,\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Cảm nhận của bài đánh giá sản phẩm sau đây là gì?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Hiệu năng tuyệt vời, điện thoại chạy mượt mà và nhanh chóng khi sử dụng các ứng dụng nặng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Điện thoại thông minh mới này cung cấp hiệu suất tuyệt vời, nhưng thời lượng pin của nó có thể tốt hơn.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Chỉ được trả ra đầu ra câu hỏi và không có giải thích gì. Dịch câu sau từ tiếng Anh sang tiếng Việt?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "The new smartphone model offers excellent performance, but its battery life could be better.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Người này phải là gái. Vì mọi cô gái không chơi trò chơi điện tử, người này phải là gái.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Suy luận câu trả lời từ câu chuyện này?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "### Câu chuyện:\n",
    "Người trong lớp này là trai hoặc gái. Mọi cậu bé chỉ chơi trò chơi điện tử và các cô gái không chơi trò chơi điện tử. Những người chơi trò chơi điện tử phải có máy tính xách tay chơi game.\n",
    "### Câu hỏi:\n",
    "Một người trong lớp này có máy tính xách tay chơi game. Người này là trai hay gái?\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kepco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
