{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/nmquy/KEPCO/kepco/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 05:43:24.186840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-12 05:43:24.209570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-12 05:43:24.216283: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-12 05:43:24.232894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-12 05:43:25.661250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/nmquy/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.586 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir=\"/workspace/nmquy/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Restaurant Employees</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Restaurant Employees</h1>\n",
      "    <table>\n",
      "        <thead>\n",
      "            <tr>\n",
      "                <th>Name</th>\n",
      "                <th>Email</th>\n",
      "            </tr>\n",
      "        </thead>\n",
      "        <tbody>\n",
      "            {% for employee in restaurant_employees %}\n",
      "                <tr>\n",
      "                    <td>{{ employee['name'] }}</td>\n",
      "                    <td>{{ employee['email'] }}</td>\n",
      "                </tr>\n",
      "            {% endfor %}\n",
      "        </tbody>\n",
      "    </table>\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "Note: This response uses the Jinja2 templating language, which is a popular templating engine\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Chuyển dictionary python sau từ JSON sang bảng HTML có header và title cột. Chỉ sử dụng mã HTML:\"\n",
    "input_data = '''\n",
    "{'resturant employees': [{'name': 'Shyam', 'email': 'shyamjaiswal@gmail.com'}, {'name': 'Bob', 'email': 'bob32@gmail.com'}, {'name': 'Jai', 'email': 'jai87@gmail.com'}]}\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot rewrite question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What NLP tasks does the BERT (Bidirectional Encoder Representations from Transformers) model excel at?</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"You are able to reason from previous conversation and the recent question, to come up with a rewrite of the question which is concise but with enough information that people without knowledge of previous conversation can understand the question:\"\n",
    "input_data = '''\n",
    "## Previous conversation\n",
    "user: What is BERT?\n",
    "assistant: BERT stands for \"Bidirectional Encoder Representations from Transformers.\" It is a natural language processing (NLP) model developed by Google. \n",
    "user: What data was used for its training?\n",
    "assistant: The BERT (Bidirectional Encoder Representations from Transformers) model was trained on a large corpus of publicly available text from the internet. It was trained on a combination of books, articles, websites, and other sources to learn the language patterns and relationships between words.\n",
    "## Question\n",
    "user: What NLP tasks can it perform well?\n",
    "## Rewritten question\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bước 1 - Lấy một số nước sôi.\n",
      "Bước 2 - Lấy một chiếc cốc và đặt một túi trà vào đó.\n",
      "Bước 3 - Đổ nước nóng lên túi trà.\n",
      "Bước 4 - Để túi trà ngồi một chút để trà có thể ngâm.\n",
      "Bước 5 - Lấy túi trà ra.\n",
      "Bước 6 - (Optional) Thêm một ít đường hoặc sữa cho vừa ăn.\n",
      "Bước 7 - Thế là xong! Bạn đã có cho mình một món ngon tách trà để thưởng thức.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Bạn sẽ được cung cấp văn bản. Nếu nó chứa một chuỗi các hướng dẫn, viết lại các hướng dẫn đó theo định dạng sau:\n",
    "Bước 1 - ...\n",
    "Bước 2 -…\n",
    "…\n",
    "Bước N -…\n",
    "\n",
    "Nếu văn bản không chứa một chuỗi hướng dẫn, sau đó chỉ cần viết \"No steps provided.\\\"\"\"\"\n",
    "input_data = '''\n",
    "Làm một tách trà thật dễ dàng! Đầu tiên, bạn cần lấy một số  nước sôi. Trong khi điều đó đang xảy ra,  \n",
    "lấy một chiếc cốc và đặt một túi trà vào đó. Một khi nước đã đủ nóng, chỉ cần đổ nó lên túi trà.  \n",
    "Hãy để nó ngồi một chút để trà có thể ngâm. Sau một vài phút, lấy túi trà ra. Nếu bạn  \n",
    "như, bạn có thể thêm một ít đường hoặc sữa cho vừa ăn. Và thế là xong! Bạn đã có cho mình một món ngon tách trà để thưởng thức.\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow many instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Vụ cháy xảy ra tại kho gỗ của Công ty TNHH Vượng Hà ở xã Cương Sơn, huyện Lục Nam vào khoảng 15h. Ngọn lửa bùng lên và lan ra toàn bộ nhà xưởng trong vài phút. Phòng Cảnh sát phòng cháy, chữa cháy và cứu nạn cứu hộ, Công an huyện Lục Nam đã huy động 40 chiến sĩ cùng ba xe chữa cháy đến hiện trường. Sau khoảng 40 phút, ngọn lửa được dập tắt nhưng hầu hết tài sản bên trong bị thiêu rụi.\n",
      "\n",
      "2 - The fire broke out at the wood warehouse of Vượng Hà Company in Cương Sơn commune, Lục Nam district at around 15h. The fire quickly spread throughout the factory within a few minutes. The Firefighting, Rescue and Emergency Medical Services, Lục Nam district police called for 40 firefighters along with 3 fire trucks to the scene. After about 40 minutes, the fire was extinguished but most of the inventory inside was destroyed.\n",
      "\n",
      "3 - {\n",
      "  \"Vietnamese_summary\": \"Vụ cháy xảy ra tại kho gỗ của Công ty TNHH Vượng Hà ở xã Cương Sơn, huyện Lục Nam vào khoảng 15h. Ngọn lửa bùng lên và lan ra toàn bộ nhà xưởng trong vài phút. Phòng Cảnh sát phòng cháy, chữa cháy và cứu nạn cứu hộ, Công an huyện Lục Nam đã huy động 40 chiến sĩ cùng ba xe chữa cháy đến hiện trường. Sau khoảng 40 phút, ngọn lửa được dập tắt nhưng hầu hết tài sản bên trong bị thiêu rụi.\",\n",
      "  \"English_summary\": \"The fire broke out at the wood warehouse of Vượng Hà Company in Cương Sơn commune, Lục Nam district at around 15h. The fire quickly spread throughout the factory within a few minutes. The Firefighting, Rescue and Emergency Medical Services, Lục Nam district police called for 40 firefighters along with 3 fire trucks to the scene. After about 40 minutes, the fire was extinguished but most of the inventory inside was destroyed.\"\n",
      "}</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Thực hiện các hành động sau: \n",
    "1 - Tóm tắt đoạn văn bản sau.\n",
    "2 - Dịch tóm tắt sang tiếng Anh.\n",
    "3 - Xuất ra một đối tượng json chứa các khóa sau: Vietnamese_summary, English_summary.\n",
    "Tách các câu trả lời của bạn bằng dấu ngắt dòng.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Khoảng 15h, nhà kho chứa đồ gỗ Công ty TNHH Vượng Hà ở xã Cương Sơn, huyện Lục Nam bốc cháy. Một người đàn ông từ bên trong chạy ra ngoài hô hoán nhờ hỗ trợ.\n",
    "\n",
    "Tuy nhiên do vật liệu dễ cháy nên chỉ trong ít phút, ngọn lửa đã bùng lên bất chấp nỗ lực chữa cháy của người dân. Khoảng 15 phút sau, ngọn lửa đã lan ra toàn bộ nhà xưởng, cột cói nghi ngút bốc cao hàng chục mét.\n",
    "Phòng Cảnh sát phòng cháy, chữa cháy và cứu nạn cứu hộ, Công an huyện Lục Nam đã huy động 40 chiến sĩ cùng ba xe chữa cháy đến hiện trường. Sau khoảng 40 phút, ngọn lửa được dập tắt nhưng hầu hết tài sản bên trong bị thiêu rụi.\n",
    "\n",
    "Ông Nguyễn Mạnh Hưng, Chủ tịch UBND xã Cương Sơn, cho biết vụ cháy không gây thiệt hại về người. Tuy nhiên, xưởng gỗ rộng khoảng 400 m2 nằm sát đường tỉnh 293 đã bị thiêu rụi. Toàn bộ phần mái tôn và cấu kiện bên trong sập đổ, tường tôn bị uốn cong, biến dạng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Học sinh đã giải quyết đúng câu hỏi. Họ đã tính toán chi phí cho năm đầu tiên hoạt động của một tòa nhà văn phòng theo diện tích tòa nhà và đã đưa ra kết quả đúng.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Xác định xem cách giải quyết của học sinh có đúng hay không.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Câu hỏi: Tôi đang lên kế hoạch xây dựng một tòa nhà văn phòng và cần tính toán chi phí cho năm đầu tiên hoạt động.\n",
    "\n",
    "Giá đất là 200 USD/m2.\n",
    "Tôi có thể mua vật liệu xây dựng với giá 300 USD/m2.\n",
    "Chi phí bảo trì hằng năm là 50 nghìn USD cho toàn bộ tòa nhà và thêm 15 USD/m2. Tổng chi phí cho năm đầu tiên hoạt động là bao nhiêu, như một hàm của diện tích tòa nhà tính bằng mét vuông.\n",
    "Giải pháp của học sinh: Gọi y là diện tích của tòa nhà tính bằng mét vuông.\n",
    "\n",
    "Chi phí:\n",
    "Giá đất: 200y USD\n",
    "Giá vật liệu xây dựng: 300y USD\n",
    "Chi phí bảo trì: 50.000+15y USD\n",
    "Tổng chi phí: \n",
    "200y+300y+50.000+15y=515y+50.000 (USD)\n",
    "Tổng chi phí cho năm đầu tiên sẽ là : 515y+50.000 USD, với y là diện tích của tòa nhà.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Một chiếc điện thoại thông minh được đánh giá cao vì hiệu năng mạnh mẽ, tốc độ xử lý nhanh và mượt mà, thời lượng pin dài, màn hình sắc nét và tính năng bảo mật vân tay tiện lợi. Tuy nhiên, camera khô\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Nhiệm vụ của bạn là tạo một bản tóm tắt ngắn về đánh giá sản phẩm từ một trang web thương mại điện tử. \n",
    "Tóm tắt đánh giá dưới đây tối đa 30 từ.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Chiếc điện thoại thông minh nhận được nhiều đánh giá tích cực từ khách hàng trên trang web thương mại điện tử. Họ khen ngợi hiệu năng mạnh mẽ, tốc độ xử lý nhanh và mượt mà, cùng với thời lượng pin ấn tượng, có thể sử dụng cả ngày mà không cần sạc. Màn hình sắc nét và tính năng bảo mật vân tay tiện lợi cũng là những điểm cộng. Tuy nhiên, một số khách hàng cho rằng camera chưa thực sự tốt trong điều kiện thiếu sáng, thiết kế của điện thoại hơi cồng kềnh, và loa ngoài không đủ lớn. Với mức giá hợp lý, sản phẩm này vẫn được đánh giá cao về chất lượng tổng thể.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cảm nhận của bài đánh giá sản phẩm là điện thoại có hiệu năng tuyệt vời, chạy mượt mà và nhanh chóng khi sử dụng các ứng dụng nặng.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Cảm nhận của bài đánh giá sản phẩm sau đây là gì?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Hiệu năng tuyệt vời, điện thoại chạy mượt mà và nhanh chóng khi sử dụng các ứng dụng nặng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mẫu điện thoại mới của hãng có khả năng hoạt động tuyệt vời, nhưng cuộc sống pin có thể tốt hơn.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Chỉ được trả ra đầu ra câu hỏi và không có giải thích gì. Dịch câu sau từ tiếng Anh sang tiếng Việt?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "The new smartphone model offers excellent performance, but its battery life could be better.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Người đó có thể là trai hoặc gái, nhưng chỉ được biết rằng họ có máy tính xách tay chơi game.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Suy luận câu trả lời từ câu chuyện này?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "### Câu chuyện:\n",
    "Người trong lớp này là trai hoặc gái. Mọi cậu bé chỉ chơi trò chơi điện tử và các cô gái không chơi trò chơi điện tử. Những người chơi trò chơi điện tử phải có máy tính xách tay chơi game.\n",
    "### Câu hỏi:\n",
    "Một người trong lớp này có máy tính xách tay chơi game. Người này là trai hay gái?\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.4, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kepco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
