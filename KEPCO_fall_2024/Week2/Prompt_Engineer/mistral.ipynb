{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/nmquy/KEPCO/kepco/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 05:43:24.186840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-12 05:43:24.209570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-12 05:43:24.216283: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-12 05:43:24.232894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-12 05:43:25.661250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/nmquy/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.586 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir=\"/workspace/nmquy/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Restaurant Employees</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Restaurant Employees</h1>\n",
      "    <table>\n",
      "        <thead>\n",
      "            <tr>\n",
      "                <th>Name</th>\n",
      "                <th>Email</th>\n",
      "            </tr>\n",
      "        </thead>\n",
      "        <tbody>\n",
      "            {% for employee in restaurant_employees %}\n",
      "                <tr>\n",
      "                    <td>{{ employee['name'] }}</td>\n",
      "                    <td>{{ employee['email'] }}</td>\n",
      "                </tr>\n",
      "            {% endfor %}\n",
      "        </tbody>\n",
      "    </table>\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "Note: This response uses the Jinja2 templating language, which is a popular templating engine\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Chuy·ªÉn dictionary python sau t·ª´ JSON sang b·∫£ng HTML c√≥ header v√† title c·ªôt. Ch·ªâ s·ª≠ d·ª•ng m√£ HTML:\"\n",
    "input_data = '''\n",
    "{'resturant employees': [{'name': 'Shyam', 'email': 'shyamjaiswal@gmail.com'}, {'name': 'Bob', 'email': 'bob32@gmail.com'}, {'name': 'Jai', 'email': 'jai87@gmail.com'}]}\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot rewrite question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What NLP tasks does the BERT (Bidirectional Encoder Representations from Transformers) model excel at?</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"You are able to reason from previous conversation and the recent question, to come up with a rewrite of the question which is concise but with enough information that people without knowledge of previous conversation can understand the question:\"\n",
    "input_data = '''\n",
    "## Previous conversation\n",
    "user: What is BERT?\n",
    "assistant: BERT stands for \"Bidirectional Encoder Representations from Transformers.\" It is a natural language processing (NLP) model developed by Google. \n",
    "user: What data was used for its training?\n",
    "assistant: The BERT (Bidirectional Encoder Representations from Transformers) model was trained on a large corpus of publicly available text from the internet. It was trained on a combination of books, articles, websites, and other sources to learn the language patterns and relationships between words.\n",
    "## Question\n",
    "user: What NLP tasks can it perform well?\n",
    "## Rewritten question\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B∆∞·ªõc 1 - L·∫•y m·ªôt s·ªë n∆∞·ªõc s√¥i.\n",
      "B∆∞·ªõc 2 - L·∫•y m·ªôt chi·∫øc c·ªëc v√† ƒë·∫∑t m·ªôt t√∫i tr√† v√†o ƒë√≥.\n",
      "B∆∞·ªõc 3 - ƒê·ªï n∆∞·ªõc n√≥ng l√™n t√∫i tr√†.\n",
      "B∆∞·ªõc 4 - ƒê·ªÉ t√∫i tr√† ng·ªìi m·ªôt ch√∫t ƒë·ªÉ tr√† c√≥ th·ªÉ ng√¢m.\n",
      "B∆∞·ªõc 5 - L·∫•y t√∫i tr√† ra.\n",
      "B∆∞·ªõc 6 - (Optional) Th√™m m·ªôt √≠t ƒë∆∞·ªùng ho·∫∑c s·ªØa cho v·ª´a ƒÉn.\n",
      "B∆∞·ªõc 7 - Th·∫ø l√† xong! B·∫°n ƒë√£ c√≥ cho m√¨nh m·ªôt m√≥n ngon t√°ch tr√† ƒë·ªÉ th∆∞·ªüng th·ª©c.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p vƒÉn b·∫£n. N·∫øu n√≥ ch·ª©a m·ªôt chu·ªói c√°c h∆∞·ªõng d·∫´n, vi·∫øt l·∫°i c√°c h∆∞·ªõng d·∫´n ƒë√≥ theo ƒë·ªãnh d·∫°ng sau:\n",
    "B∆∞·ªõc 1 - ...\n",
    "B∆∞·ªõc 2 -‚Ä¶\n",
    "‚Ä¶\n",
    "B∆∞·ªõc N -‚Ä¶\n",
    "\n",
    "N·∫øu vƒÉn b·∫£n kh√¥ng ch·ª©a m·ªôt chu·ªói h∆∞·ªõng d·∫´n, sau ƒë√≥ ch·ªâ c·∫ßn vi·∫øt \"No steps provided.\\\"\"\"\"\n",
    "input_data = '''\n",
    "L√†m m·ªôt t√°ch tr√† th·∫≠t d·ªÖ d√†ng! ƒê·∫ßu ti√™n, b·∫°n c·∫ßn l·∫•y m·ªôt s·ªë  n∆∞·ªõc s√¥i. Trong khi ƒëi·ªÅu ƒë√≥ ƒëang x·∫£y ra,  \n",
    "l·∫•y m·ªôt chi·∫øc c·ªëc v√† ƒë·∫∑t m·ªôt t√∫i tr√† v√†o ƒë√≥. M·ªôt khi n∆∞·ªõc ƒë√£ ƒë·ªß n√≥ng, ch·ªâ c·∫ßn ƒë·ªï n√≥ l√™n t√∫i tr√†.  \n",
    "H√£y ƒë·ªÉ n√≥ ng·ªìi m·ªôt ch√∫t ƒë·ªÉ tr√† c√≥ th·ªÉ ng√¢m. Sau m·ªôt v√†i ph√∫t, l·∫•y t√∫i tr√† ra. N·∫øu b·∫°n  \n",
    "nh∆∞, b·∫°n c√≥ th·ªÉ th√™m m·ªôt √≠t ƒë∆∞·ªùng ho·∫∑c s·ªØa cho v·ª´a ƒÉn. V√† th·∫ø l√† xong! B·∫°n ƒë√£ c√≥ cho m√¨nh m·ªôt m√≥n ngon t√°ch tr√† ƒë·ªÉ th∆∞·ªüng th·ª©c.\n",
    "'''\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow many instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - V·ª• ch√°y x·∫£y ra t·∫°i kho g·ªó c·ªßa C√¥ng ty TNHH V∆∞·ª£ng H√† ·ªü x√£ C∆∞∆°ng S∆°n, huy·ªán L·ª•c Nam v√†o kho·∫£ng 15h. Ng·ªçn l·ª≠a b√πng l√™n v√† lan ra to√†n b·ªô nh√† x∆∞·ªüng trong v√†i ph√∫t. Ph√≤ng C·∫£nh s√°t ph√≤ng ch√°y, ch·ªØa ch√°y v√† c·ª©u n·∫°n c·ª©u h·ªô, C√¥ng an huy·ªán L·ª•c Nam ƒë√£ huy ƒë·ªông 40 chi·∫øn sƒ© c√πng ba xe ch·ªØa ch√°y ƒë·∫øn hi·ªán tr∆∞·ªùng. Sau kho·∫£ng 40 ph√∫t, ng·ªçn l·ª≠a ƒë∆∞·ª£c d·∫≠p t·∫Øt nh∆∞ng h·∫ßu h·∫øt t√†i s·∫£n b√™n trong b·ªã thi√™u r·ª•i.\n",
      "\n",
      "2 - The fire broke out at the wood warehouse of V∆∞·ª£ng H√† Company in C∆∞∆°ng S∆°n commune, L·ª•c Nam district at around 15h. The fire quickly spread throughout the factory within a few minutes. The Firefighting, Rescue and Emergency Medical Services, L·ª•c Nam district police called for 40 firefighters along with 3 fire trucks to the scene. After about 40 minutes, the fire was extinguished but most of the inventory inside was destroyed.\n",
      "\n",
      "3 - {\n",
      "  \"Vietnamese_summary\": \"V·ª• ch√°y x·∫£y ra t·∫°i kho g·ªó c·ªßa C√¥ng ty TNHH V∆∞·ª£ng H√† ·ªü x√£ C∆∞∆°ng S∆°n, huy·ªán L·ª•c Nam v√†o kho·∫£ng 15h. Ng·ªçn l·ª≠a b√πng l√™n v√† lan ra to√†n b·ªô nh√† x∆∞·ªüng trong v√†i ph√∫t. Ph√≤ng C·∫£nh s√°t ph√≤ng ch√°y, ch·ªØa ch√°y v√† c·ª©u n·∫°n c·ª©u h·ªô, C√¥ng an huy·ªán L·ª•c Nam ƒë√£ huy ƒë·ªông 40 chi·∫øn sƒ© c√πng ba xe ch·ªØa ch√°y ƒë·∫øn hi·ªán tr∆∞·ªùng. Sau kho·∫£ng 40 ph√∫t, ng·ªçn l·ª≠a ƒë∆∞·ª£c d·∫≠p t·∫Øt nh∆∞ng h·∫ßu h·∫øt t√†i s·∫£n b√™n trong b·ªã thi√™u r·ª•i.\",\n",
      "  \"English_summary\": \"The fire broke out at the wood warehouse of V∆∞·ª£ng H√† Company in C∆∞∆°ng S∆°n commune, L·ª•c Nam district at around 15h. The fire quickly spread throughout the factory within a few minutes. The Firefighting, Rescue and Emergency Medical Services, L·ª•c Nam district police called for 40 firefighters along with 3 fire trucks to the scene. After about 40 minutes, the fire was extinguished but most of the inventory inside was destroyed.\"\n",
      "}</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Th·ª±c hi·ªán c√°c h√†nh ƒë·ªông sau: \n",
    "1 - T√≥m t·∫Øt ƒëo·∫°n vƒÉn b·∫£n sau.\n",
    "2 - D·ªãch t√≥m t·∫Øt sang ti·∫øng Anh.\n",
    "3 - Xu·∫•t ra m·ªôt ƒë·ªëi t∆∞·ª£ng json ch·ª©a c√°c kh√≥a sau: Vietnamese_summary, English_summary.\n",
    "T√°ch c√°c c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n b·∫±ng d·∫•u ng·∫Øt d√≤ng.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Kho·∫£ng 15h, nh√† kho ch·ª©a ƒë·ªì g·ªó C√¥ng ty TNHH V∆∞·ª£ng H√† ·ªü x√£ C∆∞∆°ng S∆°n, huy·ªán L·ª•c Nam b·ªëc ch√°y. M·ªôt ng∆∞·ªùi ƒë√†n √¥ng t·ª´ b√™n trong ch·∫°y ra ngo√†i h√¥ ho√°n nh·ªù h·ªó tr·ª£.\n",
    "\n",
    "Tuy nhi√™n do v·∫≠t li·ªáu d·ªÖ ch√°y n√™n ch·ªâ trong √≠t ph√∫t, ng·ªçn l·ª≠a ƒë√£ b√πng l√™n b·∫•t ch·∫•p n·ªó l·ª±c ch·ªØa ch√°y c·ªßa ng∆∞·ªùi d√¢n. Kho·∫£ng 15 ph√∫t sau, ng·ªçn l·ª≠a ƒë√£ lan ra to√†n b·ªô nh√† x∆∞·ªüng, c·ªôt c√≥i nghi ng√∫t b·ªëc cao h√†ng ch·ª•c m√©t.\n",
    "Ph√≤ng C·∫£nh s√°t ph√≤ng ch√°y, ch·ªØa ch√°y v√† c·ª©u n·∫°n c·ª©u h·ªô, C√¥ng an huy·ªán L·ª•c Nam ƒë√£ huy ƒë·ªông 40 chi·∫øn sƒ© c√πng ba xe ch·ªØa ch√°y ƒë·∫øn hi·ªán tr∆∞·ªùng. Sau kho·∫£ng 40 ph√∫t, ng·ªçn l·ª≠a ƒë∆∞·ª£c d·∫≠p t·∫Øt nh∆∞ng h·∫ßu h·∫øt t√†i s·∫£n b√™n trong b·ªã thi√™u r·ª•i.\n",
    "\n",
    "√îng Nguy·ªÖn M·∫°nh H∆∞ng, Ch·ªß t·ªãch UBND x√£ C∆∞∆°ng S∆°n, cho bi·∫øt v·ª• ch√°y kh√¥ng g√¢y thi·ªát h·∫°i v·ªÅ ng∆∞·ªùi. Tuy nhi√™n, x∆∞·ªüng g·ªó r·ªông kho·∫£ng 400 m2 n·∫±m s√°t ƒë∆∞·ªùng t·ªânh 293 ƒë√£ b·ªã thi√™u r·ª•i. To√†n b·ªô ph·∫ßn m√°i t√¥n v√† c·∫•u ki·ªán b√™n trong s·∫≠p ƒë·ªï, t∆∞·ªùng t√¥n b·ªã u·ªën cong, bi·∫øn d·∫°ng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H·ªçc sinh ƒë√£ gi·∫£i quy·∫øt ƒë√∫ng c√¢u h·ªèi. H·ªç ƒë√£ t√≠nh to√°n chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông c·ªßa m·ªôt t√≤a nh√† vƒÉn ph√≤ng theo di·ªán t√≠ch t√≤a nh√† v√† ƒë√£ ƒë∆∞a ra k·∫øt qu·∫£ ƒë√∫ng.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"X√°c ƒë·ªãnh xem c√°ch gi·∫£i quy·∫øt c·ªßa h·ªçc sinh c√≥ ƒë√∫ng hay kh√¥ng.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "C√¢u h·ªèi: T√¥i ƒëang l√™n k·∫ø ho·∫°ch x√¢y d·ª±ng m·ªôt t√≤a nh√† vƒÉn ph√≤ng v√† c·∫ßn t√≠nh to√°n chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông.\n",
    "\n",
    "Gi√° ƒë·∫•t l√† 200 USD/m2.\n",
    "T√¥i c√≥ th·ªÉ mua v·∫≠t li·ªáu x√¢y d·ª±ng v·ªõi gi√° 300 USD/m2.\n",
    "Chi ph√≠ b·∫£o tr√¨ h·∫±ng nƒÉm l√† 50 ngh√¨n USD cho to√†n b·ªô t√≤a nh√† v√† th√™m 15 USD/m2. T·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n ho·∫°t ƒë·ªông l√† bao nhi√™u, nh∆∞ m·ªôt h√†m c·ªßa di·ªán t√≠ch t√≤a nh√† t√≠nh b·∫±ng m√©t vu√¥ng.\n",
    "Gi·∫£i ph√°p c·ªßa h·ªçc sinh: G·ªçi y l√† di·ªán t√≠ch c·ªßa t√≤a nh√† t√≠nh b·∫±ng m√©t vu√¥ng.\n",
    "\n",
    "Chi ph√≠:\n",
    "Gi√° ƒë·∫•t: 200y USD\n",
    "Gi√° v·∫≠t li·ªáu x√¢y d·ª±ng: 300y USD\n",
    "Chi ph√≠ b·∫£o tr√¨: 50.000+15y USD\n",
    "T·ªïng chi ph√≠: \n",
    "200y+300y+50.000+15y=515y+50.000 (USD)\n",
    "T·ªïng chi ph√≠ cho nƒÉm ƒë·∫ßu ti√™n s·∫Ω l√† : 515y+50.000 USD, v·ªõi y l√† di·ªán t√≠ch c·ªßa t√≤a nh√†.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M·ªôt chi·∫øc ƒëi·ªán tho·∫°i th√¥ng minh ƒë∆∞·ª£c ƒë√°nh gi√° cao v√¨ hi·ªáu nƒÉng m·∫°nh m·∫Ω, t·ªëc ƒë·ªô x·ª≠ l√Ω nhanh v√† m∆∞·ª£t m√†, th·ªùi l∆∞·ª£ng pin d√†i, m√†n h√¨nh s·∫Øc n√©t v√† t√≠nh nƒÉng b·∫£o m·∫≠t v√¢n tay ti·ªán l·ª£i. Tuy nhi√™n, camera kh√¥\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Nhi·ªám v·ª• c·ªßa b·∫°n l√† t·∫°o m·ªôt b·∫£n t√≥m t·∫Øt ng·∫Øn v·ªÅ ƒë√°nh gi√° s·∫£n ph·∫©m t·ª´ m·ªôt trang web th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. \n",
    "T√≥m t·∫Øt ƒë√°nh gi√° d∆∞·ªõi ƒë√¢y t·ªëi ƒëa 30 t·ª´.\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Chi·∫øc ƒëi·ªán tho·∫°i th√¥ng minh nh·∫≠n ƒë∆∞·ª£c nhi·ªÅu ƒë√°nh gi√° t√≠ch c·ª±c t·ª´ kh√°ch h√†ng tr√™n trang web th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. H·ªç khen ng·ª£i hi·ªáu nƒÉng m·∫°nh m·∫Ω, t·ªëc ƒë·ªô x·ª≠ l√Ω nhanh v√† m∆∞·ª£t m√†, c√πng v·ªõi th·ªùi l∆∞·ª£ng pin ·∫•n t∆∞·ª£ng, c√≥ th·ªÉ s·ª≠ d·ª•ng c·∫£ ng√†y m√† kh√¥ng c·∫ßn s·∫°c. M√†n h√¨nh s·∫Øc n√©t v√† t√≠nh nƒÉng b·∫£o m·∫≠t v√¢n tay ti·ªán l·ª£i c≈©ng l√† nh·ªØng ƒëi·ªÉm c·ªông. Tuy nhi√™n, m·ªôt s·ªë kh√°ch h√†ng cho r·∫±ng camera ch∆∞a th·ª±c s·ª± t·ªët trong ƒëi·ªÅu ki·ªán thi·∫øu s√°ng, thi·∫øt k·∫ø c·ªßa ƒëi·ªán tho·∫°i h∆°i c·ªìng k·ªÅnh, v√† loa ngo√†i kh√¥ng ƒë·ªß l·ªõn. V·ªõi m·ª©c gi√° h·ª£p l√Ω, s·∫£n ph·∫©m n√†y v·∫´n ƒë∆∞·ª£c ƒë√°nh gi√° cao v·ªÅ ch·∫•t l∆∞·ª£ng t·ªïng th·ªÉ.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C·∫£m nh·∫≠n c·ªßa b√†i ƒë√°nh gi√° s·∫£n ph·∫©m l√† ƒëi·ªán tho·∫°i c√≥ hi·ªáu nƒÉng tuy·ªát v·ªùi, ch·∫°y m∆∞·ª£t m√† v√† nhanh ch√≥ng khi s·ª≠ d·ª•ng c√°c ·ª©ng d·ª•ng n·∫∑ng.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"C·∫£m nh·∫≠n c·ªßa b√†i ƒë√°nh gi√° s·∫£n ph·∫©m sau ƒë√¢y l√† g√¨?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "Hi·ªáu nƒÉng tuy·ªát v·ªùi, ƒëi·ªán tho·∫°i ch·∫°y m∆∞·ª£t m√† v√† nhanh ch√≥ng khi s·ª≠ d·ª•ng c√°c ·ª©ng d·ª•ng n·∫∑ng.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M·∫´u ƒëi·ªán tho·∫°i m·ªõi c·ªßa h√£ng c√≥ kh·∫£ nƒÉng ho·∫°t ƒë·ªông tuy·ªát v·ªùi, nh∆∞ng cu·ªôc s·ªëng pin c√≥ th·ªÉ t·ªët h∆°n.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Ch·ªâ ƒë∆∞·ª£c tr·∫£ ra ƒë·∫ßu ra c√¢u h·ªèi v√† kh√¥ng c√≥ gi·∫£i th√≠ch g√¨. D·ªãch c√¢u sau t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "The new smartphone model offers excellent performance, but its battery life could be better.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ng∆∞·ªùi ƒë√≥ c√≥ th·ªÉ l√† trai ho·∫∑c g√°i, nh∆∞ng ch·ªâ ƒë∆∞·ª£c bi·∫øt r·∫±ng h·ªç c√≥ m√°y t√≠nh x√°ch tay ch∆°i game.</s>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"Suy lu·∫≠n c√¢u tr·∫£ l·ªùi t·ª´ c√¢u chuy·ªán n√†y?\"\"\"\n",
    "\n",
    "input_data = \"\"\"\n",
    "### C√¢u chuy·ªán:\n",
    "Ng∆∞·ªùi trong l·ªõp n√†y l√† trai ho·∫∑c g√°i. M·ªçi c·∫≠u b√© ch·ªâ ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠ v√† c√°c c√¥ g√°i kh√¥ng ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠. Nh·ªØng ng∆∞·ªùi ch∆°i tr√≤ ch∆°i ƒëi·ªán t·ª≠ ph·∫£i c√≥ m√°y t√≠nh x√°ch tay ch∆°i game.\n",
    "### C√¢u h·ªèi:\n",
    "M·ªôt ng∆∞·ªùi trong l·ªõp n√†y c√≥ m√°y t√≠nh x√°ch tay ch∆°i game. Ng∆∞·ªùi n√†y l√† trai hay g√°i?\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction,  # instruction\n",
    "        input_data,   # input\n",
    "        \"\"            # output \n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.4, use_cache=True)\n",
    "html_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
    "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
    "print(response_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kepco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
