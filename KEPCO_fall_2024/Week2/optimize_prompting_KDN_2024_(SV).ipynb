{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimize Prompting Technique"
      ],
      "metadata": {
        "id": "KKm2h3hdvov8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "5g4A3fG8wKFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XoRZl35wSzo",
        "outputId": "ca2fdb93-be91-4c1a-f5ec-29e48bebcdb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.51.2\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "API_KEY = ''\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=API_KEY,\n",
        ")\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "ZSAdDmMuwMky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot prompting"
      ],
      "metadata": {
        "id": "21B6GDQWvs8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNxYk6X2viY0"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "Classify the text into neutral, negative or positive.\n",
        "Text: I think the vacation is okay.\n",
        "Sentiment:'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-shot prompting"
      ],
      "metadata": {
        "id": "hoVaQfmEv6aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''\n",
        "A \"whatpu\" is a small, furry animal native to Tanzania.\n",
        "An example of a sentence that uses the word whatpu is:\n",
        "We were traveling in Africa and we saw these very cute whatpus.\n",
        "\n",
        "To do a \"farduddle\" means to jump up and down really fast.\n",
        "An example of a sentence that uses the word farduddle is:\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "rTFPgg0LvyGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain-of-Though Prompting (CoT)"
      ],
      "metadata": {
        "id": "bTBmZX_gX4pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with zero-shot\n",
        "prompt='''\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "FgX547mjZ8ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with few-shot prompting\n",
        "prompt= '''\n",
        "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "QLDxB5WcZYrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with Chain-of-Thought prompting\n",
        "prompt = '''\n",
        "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "vlx5RiScX78t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot Chain-of-Though prompting\n",
        "\n",
        "prompt='''\n",
        "I went to the market and bought 10 apples.\n",
        "I gave 2 apples to the neighbor and 2 to the repairman.\n",
        "I then went and bought 5 more apples and ate 1.\n",
        "How many apples did I remain with?\n",
        "Let's think step by step.\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "nkzG9hv-ar4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bài tập: sử dụng chiến lược self-consistency prompt để giải quyết bài toán sau\n",
        "prompt='''\n",
        "Tìm tất cả các cặp 3 số có tổng là 26\n",
        "(1, 5, 10, 20, 6, 8, 12)\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Q07--WuQyDhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Chain-of-Thought"
      ],
      "metadata": {
        "id": "XqoHH2ubfq--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When applying chain-of-thought prompting with demonstrations, the process involves hand-crafting effective and diverse examples. This manual effort could lead to suboptimal solutions. [Zhang et al. (2022)](https://arxiv.org/abs/2210.03493) propose an approach to eliminate manual efforts by leveraging LLMs with \"Let's think step by step\" prompt to generate reasoning chains for demonstrations one by one. This automatic process can still end up with mistakes in generated chains. To mitigate the effects of the mistakes, the diversity of demonstrations matter This work proposes Auto-CoT, which samples questions with diversity and generates reasoning chains to construct the demonstrations.\n",
        "\n",
        "Auto-CoT consists of two main stages:\n",
        "- Stage 1): question clustering: partition questions of a given dataset into a few clusters\n",
        "- Stage 2): demonstration sampling: select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics\n",
        "\n",
        "The simple heuristics could be length of questions (e.g., 60 tokens) and number of steps in rationale (e.g., 5 reasoning steps). This encourages the model to use simple and accurate demonstrations.\n",
        "\n",
        "<img src=\"https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fauto-cot.642d9bad.png&w=1200&q=75\" alt=\"drawing\" style=\"width:200px;\"/>"
      ],
      "metadata": {
        "id": "K5R9XCs3a7eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/amazon-science/auto-cot.git\n",
        "%cd auto-cot\n",
        "!pip install -r requirements.txt\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "from api import cot\n",
        "question = \"In a video game, each enemy defeated gives you 7 points. If a level has 11 enemies total and you destroy all but 8 of them, how many points would you earn?\"\n",
        "print(\"Example: Auto-CoT\")\n",
        "cot(method=\"auto_cot\", question=question)"
      ],
      "metadata": {
        "id": "RoOYpX0Re7bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta Prompting"
      ],
      "metadata": {
        "id": "SC1aD4TQgIN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "Problem Statement:\n",
        "Problem: [question to be answered]\n",
        "Solution Structure:\n",
        "1. Begin the response with \"Let's think step by step.\"\n",
        "2. Follow with the reasoning steps, ensuring the solution process is broken down clearly and\n",
        "logically.\n",
        "3. End the solution with the final answer encapsulated in a LaTeX-formatted box, -, for clarity\n",
        "and emphasis.\n",
        "4. Finally, state \"The answer is [final answer to the problem].\", with the final answer presented in\n",
        "LaTeX notation.\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "guaLErpnfE1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= '''\n",
        "Problem: Find the domain of the expression va-2.\n",
        "Solution: The expressions inside each square root must be non-negative. Therefore, x - 2 ≥ 0, so x ≥ 2, and 5 - x ≥ 0, so x ≤ 5. Also, the denominator cannot be equal to zero, so 5 - x > 0, which gives x < 5. Therefore, the domain of the expression is [2,5) .\n",
        "Final Answer: The final answer is 2, 5). I hope it is correct.\n",
        "Problem: If det A = 2 and det B = 12, then find det (AB).\n",
        "Solution: We have that det (AB) = (det A)(det B) = (2)(12) = 24 .\n",
        "Final Answer: The final answer is 24. I hope it is correct.\n",
        "'''"
      ],
      "metadata": {
        "id": "5lMdZM9JgqTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-consistency"
      ],
      "metadata": {
        "id": "OVampu0rjZRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "When I was 6 my sister was half my age. Now\n",
        "I’m 70 how old is my sister?\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "V894-CDmmFGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''\n",
        "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
        "there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
        "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
        "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
        "did Jason give to Denny?\n",
        "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
        "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
        "he have now?\n",
        "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
        "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
        "monday to thursday. How many computers are now in the server room?\n",
        "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
        "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
        "The answer is 29.\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
        "golf balls did he have at the end of wednesday?\n",
        "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
        "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
        "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
        "A:\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "c03UmQ5ZjipE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bài tập: sử dụng các chiến lược prompt đã học để giải quyết bài toán sau\n",
        "prompt='''\n",
        " Giờ Văn cô giáo trả bài kiểm tra.\n",
        " Bốn bạn Tuấn, Hùng, Lan, Quân ngồi cùng bàn đều đạt điểm 8 trở lên.\n",
        " Giờ ra chơi Phương hỏi điểm của 4 bạn, Tuấn trả lời :\n",
        "  - Lan không đạt điểm 10, mình và Quân không đạt điểm 9 còn Hùng không đạt điểm 8.\n",
        " Hùng thì nói :\n",
        "  - Mình không đạt điểm 10, Lan không đạt điểm 9 còn Tuấn và Quân đều không đạt điểm 8.\n",
        "\n",
        "Bạn hãy cho biết mỗi người đã đạt mấy điểm?.\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "45vwnOkPourZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Knowledge Prompting\n",
        "<img src=\"https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fgen-knowledge.055b8d37.png&w=828&q=75\" alt=\"drawing\" style=\"width:180px;\"/>"
      ],
      "metadata": {
        "id": "MM_H2bc8l9M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal prompt\n",
        "prompt='''\n",
        "Part of golf is trying to get a higher point total than others. Yes or No?\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "w-VG9BDlmVfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Knowledge generate prompt\n",
        "prompt='''\n",
        "Input: Greece is larger than mexico.\n",
        "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
        "Input: Glasses always fog up.\n",
        "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
        "Input: A fish is capable of thinking.\n",
        "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
        "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
        "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
        "Input: A rock is the same size as a pebble.\n",
        "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
        "Input: Part of golf is trying to get a higher point total than others.\n",
        "Knowledge:\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "sg0M4BTjmJ4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Knowledge obtained from LLMs\n",
        "knowledge1 = '''\n",
        "In golf, the objective is actually to have the lowest score possible, not the highest. Players aim to hit the ball into the hole with the fewest number of strokes, as each stroke is counted as a point against them. The player with the lowest total score at the end of the game is the winner.\n",
        "'''\n",
        "knowledge2='''\n",
        "While the ultimate goal in golf is to have the lowest score possible, the scoring system in golf is based on the number of strokes taken to complete each hole. The player with the lowest total number of strokes at the end of the round is the winner. So, in golf, the objective is to have the lowest score, not the highest.\n",
        "'''\n",
        "\n",
        "prompt=f'''\n",
        "Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
        "Knowledge: {knowledge1}\n",
        "Knowledge: {knowledge2}\n",
        "Explain and Answer:\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "QN0PFEelmA01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bài tập: thực hiện các chiến lược generate knowledge prompt để giải quyết câu hỏi sau:\n",
        "prompt='''\n",
        "chiều cao trung bình của cầu thủ bóng đá cao hơn một cầu thủ bóng rổ. Đúng hay sai ?\n",
        "'''\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "-f458PQ0ofP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tree-of-Thought\n",
        "<img src=\"https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e.png&w=1200&q=75\" alt=\"drawing\" style=\"width:180px;\"/>\n"
      ],
      "metadata": {
        "id": "ty6eFUZGzMe-"
      }
    }
  ]
}